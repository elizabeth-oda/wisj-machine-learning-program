{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-sB8LNlVOGG"
      },
      "source": [
        "\"\"\"\n",
        "RAVDESS Emotion Recognition Tutorial\n",
        "====================================\n",
        "\n",
        "This tutorial teaches you how to build an emotion recognition system using\n",
        "the RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) dataset.\n",
        "\n",
        "Learning Objectives:\n",
        "- Understand audio feature extraction (MFCC, Mel-spectrogram, etc.)\n",
        "- Learn LSTM neural networks for sequence classification\n",
        "- Apply machine learning to audio emotion recognition\n",
        "- Visualize and interpret audio features\n",
        "\n",
        "YOUR GOAL IS to improve the performance to acc 25%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "D6AbL_HK1642"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MZTP1yq18tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3IMkqCJM189L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYGsP27ZVYy6",
        "outputId": "d6c8d3aa-6340-49d5-cdaa-1676a0e7e6c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìö STEP 1: Installing and Importing Required Libraries\n",
            "--------------------------------------------------\n",
            "We need several Python libraries for audio processing and machine learning:\n",
            "‚Ä¢ kagglehub: Download datasets from Kaggle\n",
            "‚Ä¢ librosa: Audio analysis and feature extraction\n",
            "‚Ä¢ tensorflow: Deep learning framework\n",
            "‚Ä¢ sklearn: Machine learning utilities\n",
            "‚Ä¢ matplotlib/seaborn: Data visualization\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "‚úÖ All libraries imported successfully!\n",
            "\n",
            "Next: Let's download the RAVDESS dataset...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ========================= STEP 1: Install and Import Libraries =========================\n",
        "print(\"\\nüìö STEP 1: Installing and Importing Required Libraries\")\n",
        "print(\"-\" * 50)\n",
        "print(\"We need several Python libraries for audio processing and machine learning:\")\n",
        "print(\"‚Ä¢ kagglehub: Download datasets from Kaggle\")\n",
        "print(\"‚Ä¢ librosa: Audio analysis and feature extraction\")\n",
        "print(\"‚Ä¢ tensorflow: Deep learning framework\")\n",
        "print(\"‚Ä¢ sklearn: Machine learning utilities\")\n",
        "print(\"‚Ä¢ matplotlib/seaborn: Data visualization\")\n",
        "\n",
        "!pip install kagglehub librosa soundfile -q\n",
        "!pip install tensorflow\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"\\nNext: Let's download the RAVDESS dataset...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipq3VKAJVfqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123510b8-4081-4a2b-8010-74190d797664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìñ Understanding RAVDESS filename format:\n",
            "Example: 03-01-06-01-02-01-12.wav\n",
            "Position 1: Modality (01=full-AV, 02=video-only, 03=audio-only)\n",
            "Position 2: Vocal channel (01=speech, 02=song)\n",
            "Position 3: Emotion (01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=disgust, 08=surprised)\n",
            "Position 4: Emotional intensity (01=normal, 02=strong)\n",
            "Position 5: Statement (01='Kids are talking by the door', 02='Dogs are sitting by the door')\n",
            "Position 6: Repetition (01=1st repetition, 02=2nd repetition)\n",
            "Position 7: Actor (01-24, odd=male, even=female)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\nüìñ Understanding RAVDESS filename format:\")\n",
        "print(\"Example: 03-01-06-01-02-01-12.wav\")\n",
        "print(\"Position 1: Modality (01=full-AV, 02=video-only, 03=audio-only)\")\n",
        "print(\"Position 2: Vocal channel (01=speech, 02=song)\")\n",
        "print(\"Position 3: Emotion (01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=disgust, 08=surprised)\")\n",
        "print(\"Position 4: Emotional intensity (01=normal, 02=strong)\")\n",
        "print(\"Position 5: Statement (01='Kids are talking by the door', 02='Dogs are sitting by the door')\")\n",
        "print(\"Position 6: Repetition (01=1st repetition, 02=2nd repetition)\")\n",
        "print(\"Position 7: Actor (01-24, odd=male, even=female)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeYLK5k7ViOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118499e5-44bd-4efe-c7ba-eef8aa8c940e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚öôÔ∏è STEP 3: Setting Up Configuration Parameters\n",
            "--------------------------------------------------\n",
            "Let's define our emotion labels and audio processing parameters.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ========================= STEP 3: Configuration and Setup =========================\n",
        "print(\"\\n‚öôÔ∏è STEP 3: Setting Up Configuration Parameters\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Let's define our emotion labels and audio processing parameters.\")\n",
        "\n",
        "# Emotion mapping - this tells us what each number means\n",
        "EMOTIONS = {\n",
        "    1: 'neutral',     # No particular emotion\n",
        "    2: 'calm',        # Peaceful, relaxed\n",
        "    3: 'happy',       # Joyful, positive\n",
        "    4: 'sad',         # Sorrowful, negative\n",
        "    5: 'angry',       # Aggressive, hostile\n",
        "    6: 'fearful',     # Scared, anxious\n",
        "    7: 'disgust',     # Repulsed, disgusted\n",
        "    0: 'surprised'    # Shocked, amazed (originally 8, changed to 0)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3dHxdj8i5Je",
        "outputId": "ac92e808-169c-448b-c1a2-86acadec8aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß STEP 4: Creating Functions to Parse Filenames (DEBUG VERSION)\n",
            "--------------------------------------------------\n",
            "\n",
            "üîç Checking multiple possible dataset locations...\n",
            "============================================================\n",
            "\n",
            "‚ùå Path not found: /kaggle/input/ravdess-emotional-speech-audio\n",
            "\n",
            "‚ùå Path not found: /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24\n",
            "\n",
            "‚ùå Path not found: /kaggle/input/ravdess\n",
            "\n",
            "‚úÖ Path exists: /kaggle/input/\n",
            "\n",
            "üîç Exploring directory: /kaggle/input/\n",
            "\n",
            "üìÅ Contents of /kaggle/input/:\n",
            "\n",
            "üéµ Searching for .wav files in all subdirectories...\n",
            "\n",
            "‚ùå No .wav files found in /kaggle/input/\n",
            "\n",
            "‚úÖ Path exists: .\n",
            "\n",
            "üîç Exploring directory: .\n",
            "\n",
            "üìÅ Contents of .:\n",
            "  üìÅ .config/\n",
            "  üìÅ sample_data/\n",
            "\n",
            "üéµ Searching for .wav files in all subdirectories...\n",
            "\n",
            "‚ùå No .wav files found in .\n",
            "\n",
            "‚ùå Path not found: ./ravdess-emotional-speech-audio\n",
            "\n",
            "‚ùå Path not found: ./audio_speech_actors_01-24\n",
            "\n",
            "üîç Let's check what's available in /kaggle/input/\n",
            "\n",
            "Available datasets in /kaggle/input/:\n",
            "\n",
            "\n",
            "üí° RECOMMENDATIONS:\n",
            "============================================================\n",
            "‚ùå No RAVDESS audio files found!\n",
            "\n",
            "Possible solutions:\n",
            "1. Make sure the RAVDESS dataset is added to your Kaggle notebook\n",
            "2. Check if you need to extract a zip file first\n",
            "3. Verify the dataset name in Kaggle\n",
            "\n",
            "To add the dataset in Kaggle:\n",
            "1. Click '+ Add data' in the right sidebar\n",
            "2. Search for 'RAVDESS'\n",
            "3. Select the dataset and click 'Add'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ixJOyahIhza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "CdQrrIotVrBZ",
        "outputId": "4477d139-fd65-46dc-9902-490210ab28cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß STEP 4: Creating Functions to Parse Filenames\n",
            "--------------------------------------------------\n",
            "We need to extract emotion information from the filename structure.\n",
            "\n",
            "üß™ Testing filename parser:\n",
            "Input: 03-01-06-01-02-01-12.wav\n",
            "Output: {'emotion': 6, 'emotion_label': 'fearful', 'intensity': 1, 'statement': 2, 'repetition': 1, 'actor': 12, 'gender': 'female'}\n",
            "\n",
            "üîÑ Loading audio file information...\n",
            "üîç Searching for audio files...\n",
            "üìÅ Audio files located in: /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24\n",
            "‚úÖ Found 0 valid audio files\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "‚ùå No valid audio files found. Please check the data path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-392861127.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ùå No valid audio files found. Please check the data path.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ Successfully loaded information for {len(file_df)} audio files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ‚ùå No valid audio files found. Please check the data path."
          ]
        }
      ],
      "source": [
        "# ========================= STEP 4: Filename Parsing Functions =========================\n",
        "import os\n",
        "import pandas as pd\n",
        "print(\"\\nüîß STEP 4: Creating Functions to Parse Filenames\")\n",
        "print(\"-\" * 50)\n",
        "print(\"We need to extract emotion information from the filename structure.\")\n",
        "\n",
        "def parse_filename(filename):\n",
        "    \"\"\"\n",
        "    Parse RAVDESS filename to extract metadata.\n",
        "\n",
        "    Args:\n",
        "        filename (str): The audio filename to parse\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing parsed information, or None if invalid\n",
        "\n",
        "    Example:\n",
        "        Input: \"03-01-06-01-02-01-12.wav\"\n",
        "        Output: {'emotion': 6, 'emotion_label': 'fearful', 'intensity': 1, ...}\n",
        "    \"\"\"\n",
        "    # Remove .wav extension and split by dashes\n",
        "    parts = filename.replace('.wav', '').split('-')\n",
        "\n",
        "    # Check if we have exactly 7 parts\n",
        "    if len(parts) != 7:\n",
        "        print(f\"‚ö†Ô∏è Invalid filename format: {filename}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Extract emotion (3rd position)\n",
        "        emotion = int(parts[2])\n",
        "\n",
        "        # Convert surprise from 8 to 0 for easier processing\n",
        "        if emotion == 8:\n",
        "            emotion = 0\n",
        "\n",
        "        # Create metadata dictionary\n",
        "        metadata = {\n",
        "            'emotion': emotion,\n",
        "            'emotion_label': EMOTIONS.get(emotion, 'unknown'),\n",
        "            'intensity': int(parts[3]),      # 1=normal, 2=strong\n",
        "            'statement': int(parts[4]),      # Which sentence was spoken\n",
        "            'repetition': int(parts[5]),     # 1st or 2nd repetition\n",
        "            'actor': int(parts[6]),          # Actor ID (1-24)\n",
        "            'gender': 'female' if int(parts[6]) % 2 == 0 else 'male'  # Even=female, odd=male\n",
        "        }\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    except (ValueError, IndexError) as e:\n",
        "        print(f\"‚ö†Ô∏è Error parsing filename {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test the function\n",
        "test_filename = \"03-01-06-01-02-01-12.wav\"\n",
        "test_result = parse_filename(test_filename)\n",
        "print(f\"\\nüß™ Testing filename parser:\")\n",
        "print(f\"Input: {test_filename}\")\n",
        "print(f\"Output: {test_result}\")\n",
        "\n",
        "def load_audio_files(data_path):\n",
        "    \"\"\"\n",
        "    Load all audio files and extract their metadata.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Path to the dataset\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing file information\n",
        "    \"\"\"\n",
        "    print(\"üîç Searching for audio files...\")\n",
        "\n",
        "    # Find the audio directory\n",
        "    audio_path = os.path.join(data_path, 'audio_speech_actors_01-24')\n",
        "    if not os.path.exists(audio_path):\n",
        "        # Search for the correct directory\n",
        "        for root, dirs, files in os.walk(data_path):\n",
        "            if 'Actor_01' in dirs or any('.wav' in f for f in files):\n",
        "                audio_path = root\n",
        "                break\n",
        "\n",
        "    print(f\"üìÅ Audio files located in: {audio_path}\")\n",
        "\n",
        "    file_data = []\n",
        "\n",
        "    # Walk through all directories and files\n",
        "    for root, dirs, files in os.walk(audio_path):\n",
        "        for filename in files:\n",
        "            if filename.endswith('.wav'):\n",
        "                file_path = os.path.join(root, filename)\n",
        "                metadata = parse_filename(filename)\n",
        "\n",
        "                # Only keep files with valid emotions\n",
        "                if metadata and metadata['emotion'] in EMOTIONS:\n",
        "                    metadata['file_path'] = file_path\n",
        "                    metadata['filename'] = filename\n",
        "                    file_data.append(metadata)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(file_data)\n",
        "    print(f\"‚úÖ Found {len(df)} valid audio files\")\n",
        "\n",
        "    if len(df) > 0:\n",
        "        print(\"\\nüìà Data distribution:\")\n",
        "        print(\"Emotions:\")\n",
        "        print(df['emotion_label'].value_counts())\n",
        "        print(f\"\\nGender distribution:\")\n",
        "        print(df['gender'].value_counts())\n",
        "        print(f\"\\nIntensity distribution:\")\n",
        "        print(df['intensity'].value_counts())\n",
        "\n",
        "    return df\n",
        "\n",
        "# Define the data path\n",
        "data_path = '/kaggle/input/ravdess-emotional-speech-audio'\n",
        "\n",
        "# Load file information\n",
        "print(\"\\nüîÑ Loading audio file information...\")\n",
        "file_df = load_audio_files(data_path)\n",
        "\n",
        "if len(file_df) == 0:\n",
        "    raise ValueError(\"‚ùå No valid audio files found. Please check the data path.\")\n",
        "\n",
        "print(f\"‚úÖ Successfully loaded information for {len(file_df)} audio files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2RjXjKAVmpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf7fcb9-c7b1-4415-9b10-a1b81edaea4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Emotion categories: ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
            "üéµ Audio parameters: Sample rate=22050Hz, Duration=3.0s\n",
            "üß† Training parameters: Batch size=32, Max epochs=50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Audio processing parameters\n",
        "SAMPLE_RATE = 22050    # How many samples per second (Hz)\n",
        "DURATION = 3.0         # Length of audio to analyze (seconds)\n",
        "OFFSET = 0.5          # Skip first 0.5 seconds (remove silence)\n",
        "\n",
        "# Model training parameters\n",
        "N_MELS = 128          # Number of Mel frequency bands\n",
        "N_MFCC = 13           # Number of MFCC coefficients\n",
        "BATCH_SIZE = 32       # Number of samples per training batch\n",
        "EPOCHS = 50           # Maximum number of training epochs\n",
        "VALIDATION_SPLIT = 0.2  # 20% for validation\n",
        "TEST_SPLIT = 0.1      # 10% for testing\n",
        "\n",
        "print(f\"üìä Emotion categories: {list(EMOTIONS.values())}\")\n",
        "print(f\"üéµ Audio parameters: Sample rate={SAMPLE_RATE}Hz, Duration={DURATION}s\")\n",
        "print(f\"üß† Training parameters: Batch size={BATCH_SIZE}, Max epochs={EPOCHS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOHAwVwJV_P5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 5: Audio Feature Extraction =========================\n",
        "print(\"\\nüéµ STEP 5: Understanding Audio Feature Extraction\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Audio signals are complex. We need to extract meaningful features that\")\n",
        "print(\"capture the characteristics that distinguish different emotions.\")\n",
        "print(\"\\nKey audio features we'll extract:\")\n",
        "print(\"‚Ä¢ MFCC: Captures the shape of the spectral envelope\")\n",
        "print(\"‚Ä¢ Mel-spectrogram: Time-frequency representation of audio\")\n",
        "print(\"‚Ä¢ Chroma: Represents the 12 different pitch classes\")\n",
        "print(\"‚Ä¢ Spectral contrast: Measures the difference in amplitude between peaks and valleys\")\n",
        "print(\"‚Ä¢ Zero crossing rate: How often the signal crosses zero\")\n",
        "\n",
        "def extract_audio_features(file_path, sr=SAMPLE_RATE, duration=DURATION, offset=OFFSET, verbose=False):\n",
        "    \"\"\"\n",
        "    Extract comprehensive audio features for emotion recognition.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the audio file\n",
        "        sr (int): Sample rate\n",
        "        duration (float): Duration to load (seconds)\n",
        "        offset (float): Offset from start (seconds)\n",
        "        verbose (bool): Whether to show detailed output\n",
        "\n",
        "    Returns:\n",
        "        tuple: (features_dict, raw_audio) or (None, None) if error\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Load audio file\n",
        "        if verbose:\n",
        "            print(f\"üéµ Loading audio: {os.path.basename(file_path)}\")\n",
        "        audio, _ = librosa.load(file_path, sr=sr, duration=duration, offset=offset)\n",
        "\n",
        "        # Step 2: Ensure consistent length\n",
        "        target_length = int(sr * duration)\n",
        "        if len(audio) < target_length:\n",
        "            # Pad with zeros if too short\n",
        "            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
        "        else:\n",
        "            # Truncate if too long\n",
        "            audio = audio[:target_length]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Audio length: {len(audio)} samples ({len(audio)/sr:.2f} seconds)\")\n",
        "\n",
        "        # Step 3: Extract features\n",
        "        features = {}\n",
        "\n",
        "        if verbose:\n",
        "            print(\"   Extracting MFCC features...\")\n",
        "        # MFCC: Mel-frequency cepstral coefficients\n",
        "        # These capture the shape of the spectral envelope\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)\n",
        "        features['mfcc'] = mfccs\n",
        "        if verbose:\n",
        "            print(f\"     MFCC shape: {mfccs.shape}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(\"   Extracting Mel-spectrogram...\")\n",
        "        # Mel-spectrogram: Time-frequency representation\n",
        "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=N_MELS)\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "        features['mel_spectrogram'] = mel_spec_db\n",
        "        if verbose:\n",
        "            print(f\"     Mel-spectrogram shape: {mel_spec_db.shape}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(\"   Extracting Chroma features...\")\n",
        "        # Chroma: Represents the 12 different pitch classes\n",
        "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
        "        features['chroma'] = chroma\n",
        "        if verbose:\n",
        "            print(f\"     Chroma shape: {chroma.shape}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(\"   Extracting Spectral contrast...\")\n",
        "        # Spectral contrast: Difference between peaks and valleys in spectrum\n",
        "        contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
        "        features['spectral_contrast'] = contrast\n",
        "        if verbose:\n",
        "            print(f\"     Spectral contrast shape: {contrast.shape}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(\"   Extracting Zero crossing rate...\")\n",
        "        # Zero crossing rate: How often signal crosses zero\n",
        "        zcr = librosa.feature.zero_crossing_rate(audio)\n",
        "        features['zcr'] = zcr\n",
        "        if verbose:\n",
        "            print(f\"     ZCR shape: {zcr.shape}\")\n",
        "\n",
        "        return features, audio\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def prepare_lstm_features(features):\n",
        "    \"\"\"\n",
        "    Prepare features for LSTM input.\n",
        "    LSTM needs input in format: (time_steps, features)\n",
        "\n",
        "    Args:\n",
        "        features (dict): Dictionary of extracted features\n",
        "\n",
        "    Returns:\n",
        "        np.array: Features formatted for LSTM\n",
        "    \"\"\"\n",
        "    # Use Mel-spectrogram as our main feature\n",
        "    mel_spec = features['mel_spectrogram']  # Shape: (n_mels, time_frames)\n",
        "\n",
        "    # Transpose to get (time_frames, n_mels) for LSTM\n",
        "    mel_spec = mel_spec.T\n",
        "\n",
        "    return mel_spec\n",
        "\n",
        "# Test feature extraction on one file\n",
        "print(\"\\nüß™ Testing feature extraction on a sample file...\")\n",
        "test_file = file_df.iloc[0]\n",
        "print(f\"Test file: {test_file['filename']} (Emotion: {test_file['emotion_label']})\")\n",
        "\n",
        "test_features, test_audio = extract_audio_features(test_file['file_path'], verbose=True)\n",
        "if test_features:\n",
        "    test_lstm_features = prepare_lstm_features(test_features)\n",
        "    print(f\"‚úÖ LSTM features shape: {test_lstm_features.shape}\")\n",
        "    print(\"   Format: (time_frames, mel_frequency_bands)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GABF0bKoWG-o"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 6: Data Augmentation Functions =========================\n",
        "print(\"\\nüîÑ STEP 6: Data Augmentation Techniques\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Data augmentation helps improve model performance by creating variations\")\n",
        "print(\"of our training data. This helps the model generalize better.\")\n",
        "\n",
        "def add_noise(audio, noise_factor=0.005):\n",
        "    \"\"\"\n",
        "    Add Gaussian white noise to audio signal.\n",
        "    This simulates real-world recording conditions.\n",
        "\n",
        "    Args:\n",
        "        audio (np.array): Original audio signal\n",
        "        noise_factor (float): How much noise to add (0.005 = 0.5%)\n",
        "\n",
        "    Returns:\n",
        "        np.array: Audio with added noise\n",
        "    \"\"\"\n",
        "    noise = np.random.normal(0, noise_factor, len(audio))\n",
        "    return audio + noise\n",
        "\n",
        "def shift_audio(audio, shift_max=0.2):\n",
        "    \"\"\"\n",
        "    Shift audio in time (circular shift).\n",
        "    This simulates different timing in speech.\n",
        "\n",
        "    Args:\n",
        "        audio (np.array): Original audio signal\n",
        "        shift_max (float): Maximum shift as fraction of audio length\n",
        "\n",
        "    Returns:\n",
        "        np.array: Time-shifted audio\n",
        "    \"\"\"\n",
        "    shift = np.random.randint(-int(len(audio) * shift_max), int(len(audio) * shift_max))\n",
        "    return np.roll(audio, shift)\n",
        "\n",
        "# Demonstrate data augmentation\n",
        "print(\"\\nüß™ Demonstrating data augmentation:\")\n",
        "original_audio = test_audio\n",
        "noisy_audio = add_noise(original_audio)\n",
        "shifted_audio = shift_audio(original_audio)\n",
        "\n",
        "print(f\"Original audio range: [{original_audio.min():.4f}, {original_audio.max():.4f}]\")\n",
        "print(f\"Noisy audio range: [{noisy_audio.min():.4f}, {noisy_audio.max():.4f}]\")\n",
        "print(f\"Shifted audio range: [{shifted_audio.min():.4f}, {shifted_audio.max():.4f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFD9_xiAWKOk"
      },
      "outputs": [],
      "source": [
        "# ========================= STEP 7: Feature Visualization Functions =========================\n",
        "print(\"\\nüìä STEP 7: Creating Feature Visualization Functions\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Visualization helps us understand what our features look like and\")\n",
        "print(\"how different emotions appear in the feature space.\")\n",
        "\n",
        "def visualize_audio_features(features, audio, emotion_label, filename, sample_rate=SAMPLE_RATE):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualization of audio features.\n",
        "\n",
        "    Args:\n",
        "        features (dict): Extracted audio features\n",
        "        audio (np.array): Raw audio signal\n",
        "        emotion_label (str): Emotion name\n",
        "        filename (str): Audio filename\n",
        "        sample_rate (int): Audio sample rate\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    fig.suptitle(f'Audio Feature Visualization - {emotion_label.upper()} ({filename})', fontsize=16)\n",
        "\n",
        "    # 1. Raw audio waveform\n",
        "    time_axis = np.linspace(0, len(audio)/sample_rate, len(audio))\n",
        "    axes[0, 0].plot(time_axis, audio, color='blue', alpha=0.7)\n",
        "    axes[0, 0].set_title('Raw Audio Waveform')\n",
        "    axes[0, 0].set_xlabel('Time (seconds)')\n",
        "    axes[0, 0].set_ylabel('Amplitude')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. MFCC features\n",
        "    mfcc = features['mfcc']\n",
        "    img1 = axes[0, 1].imshow(mfcc, aspect='auto', origin='lower', cmap='viridis')\n",
        "    axes[0, 1].set_title(f'MFCC Features ({mfcc.shape[0]} x {mfcc.shape[1]})')\n",
        "    axes[0, 1].set_xlabel('Time Frames')\n",
        "    axes[0, 1].set_ylabel('MFCC Coefficients')\n",
        "    plt.colorbar(img1, ax=axes[0, 1])\n",
        "\n",
        "    # 3. Mel-spectrogram\n",
        "    mel_spec = features['mel_spectrogram']\n",
        "    img2 = axes[0, 2].imshow(mel_spec, aspect='auto', origin='lower', cmap='magma')\n",
        "    axes[0, 2].set_title(f'Mel-Spectrogram ({mel_spec.shape[0]} x {mel_spec.shape[1]})')\n",
        "    axes[0, 2].set_xlabel('Time Frames')\n",
        "    axes[0, 2].set_ylabel('Mel Frequency Bands')\n",
        "    plt.colorbar(img2, ax=axes[0, 2])\n",
        "\n",
        "    # 4. Chroma features\n",
        "    chroma = features['chroma']\n",
        "    img3 = axes[1, 0].imshow(chroma, aspect='auto', origin='lower', cmap='coolwarm')\n",
        "    axes[1, 0].set_title(f'Chroma Features ({chroma.shape[0]} x {chroma.shape[1]})')\n",
        "    axes[1, 0].set_xlabel('Time Frames')\n",
        "    axes[1, 0].set_ylabel('Pitch Classes')\n",
        "    plt.colorbar(img3, ax=axes[1, 0])\n",
        "\n",
        "    # 5. Spectral contrast\n",
        "    contrast = features['spectral_contrast']\n",
        "    img4 = axes[1, 1].imshow(contrast, aspect='auto', origin='lower', cmap='plasma')\n",
        "    axes[1, 1].set_title(f'Spectral Contrast ({contrast.shape[0]} x {contrast.shape[1]})')\n",
        "    axes[1, 1].set_xlabel('Time Frames')\n",
        "    axes[1, 1].set_ylabel('Frequency Bands')\n",
        "    plt.colorbar(img4, ax=axes[1, 1])\n",
        "\n",
        "    # 6. Zero crossing rate\n",
        "    zcr = features['zcr']\n",
        "    axes[1, 2].plot(zcr[0], color='red', linewidth=2)\n",
        "    axes[1, 2].set_title(f'Zero Crossing Rate (Length: {zcr.shape[1]})')\n",
        "    axes[1, 2].set_xlabel('Time Frames')\n",
        "    axes[1, 2].set_ylabel('ZCR')\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print feature statistics\n",
        "    print(f\"\\nüìä Feature Statistics for {emotion_label.upper()}:\")\n",
        "    print(f\"   MFCC: mean={np.mean(mfcc):.3f}, std={np.std(mfcc):.3f}\")\n",
        "    print(f\"   Mel-spectrogram: mean={np.mean(mel_spec):.3f}, std={np.std(mel_spec):.3f}\")\n",
        "    print(f\"   Chroma: mean={np.mean(chroma):.3f}, std={np.std(chroma):.3f}\")\n",
        "    print(f\"   Spectral contrast: mean={np.mean(contrast):.3f}, std={np.std(contrast):.3f}\")\n",
        "    print(f\"   Zero crossing rate: mean={np.mean(zcr):.6f}, std={np.std(zcr):.6f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "def compare_emotions_features(all_features_samples, emotion_labels, sample_indices):\n",
        "    \"\"\"\n",
        "    Compare features across different emotions.\n",
        "\n",
        "    Args:\n",
        "        all_features_samples (list): List of feature dictionaries\n",
        "        emotion_labels (list): List of emotion names\n",
        "        sample_indices (list): List of sample indices\n",
        "    \"\"\"\n",
        "    n_emotions = len(sample_indices)\n",
        "    fig, axes = plt.subplots(n_emotions, 3, figsize=(15, 4*n_emotions))\n",
        "\n",
        "    if n_emotions == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    fig.suptitle('Feature Comparison Across Emotions', fontsize=16)\n",
        "\n",
        "    for i, (features, emotion, idx) in enumerate(zip(all_features_samples, emotion_labels, sample_indices)):\n",
        "        # MFCC comparison\n",
        "        img1 = axes[i, 0].imshow(features['mfcc'], aspect='auto', origin='lower', cmap='viridis')\n",
        "        axes[i, 0].set_title(f'{emotion.upper()} - MFCC')\n",
        "        axes[i, 0].set_ylabel('MFCC Coefficients')\n",
        "        if i == n_emotions - 1:\n",
        "            axes[i, 0].set_xlabel('Time Frames')\n",
        "\n",
        "        # Mel-spectrogram comparison\n",
        "        img2 = axes[i, 1].imshow(features['mel_spectrogram'], aspect='auto', origin='lower', cmap='magma')\n",
        "        axes[i, 1].set_title(f'{emotion.upper()} - Mel-Spectrogram')\n",
        "        axes[i, 1].set_ylabel('Mel Frequency Bands')\n",
        "        if i == n_emotions - 1:\n",
        "            axes[i, 1].set_xlabel('Time Frames')\n",
        "\n",
        "        # Chroma comparison\n",
        "        img3 = axes[i, 2].imshow(features['chroma'], aspect='auto', origin='lower', cmap='coolwarm')\n",
        "        axes[i, 2].set_title(f'{emotion.upper()} - Chroma')\n",
        "        axes[i, 2].set_ylabel('Pitch Classes')\n",
        "        if i == n_emotions - 1:\n",
        "            axes[i, 2].set_xlabel('Time Frames')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaCap3v8WSJ0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 8: Batch Processing Audio Files =========================\n",
        "print(\"\\n‚öôÔ∏è STEP 8: Processing All Audio Files\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Now we'll process all audio files and extract features.\")\n",
        "print(\"This may take a few minutes depending on the dataset size.\")\n",
        "\n",
        "X_features = []           # Will store LSTM-ready features\n",
        "X_raw_audio = []         # Will store raw audio for visualization\n",
        "y_labels = []            # Will store emotion labels\n",
        "file_info = []           # Will store file metadata\n",
        "all_features_for_viz = [] # Will store complete features for visualization\n",
        "\n",
        "# For visualization samples\n",
        "viz_samples = {}  # {emotion_label: [indices]}\n",
        "samples_per_emotion = 2  # Show 2 samples per emotion\n",
        "\n",
        "print(f\"üîÑ Processing {len(file_df)} audio files...\")\n",
        "\n",
        "processed_count = 0\n",
        "augmented_count = 0\n",
        "\n",
        "# choose one methods from 4 methods\n",
        "def choose_one_augment_method(audio):\n",
        "  for idx, row in file_df.iterrows():\n",
        "    # Extract features from audio file (without verbose output)\n",
        "    features, audio = extract_audio_features(row['file_path'], verbose=False)\n",
        "\n",
        "    if features is not None:\n",
        "        # Prepare features for LSTM\n",
        "        lstm_features = prepare_lstm_features(features)\n",
        "\n",
        "        # Store everything\n",
        "        X_features.append(lstm_features)\n",
        "        X_raw_audio.append(audio)\n",
        "        y_labels.append(row['emotion'])\n",
        "        file_info.append(row)\n",
        "        all_features_for_viz.append(features)\n",
        "        processed_count += 1\n",
        "\n",
        "        # Collect samples for visualization\n",
        "        emotion_label = row['emotion_label']\n",
        "        if emotion_label not in viz_samples:\n",
        "            viz_samples[emotion_label] = []\n",
        "        if len(viz_samples[emotion_label]) < samples_per_emotion:\n",
        "            viz_samples[emotion_label].append(len(X_features) - 1)\n",
        "\n",
        "        # Data augmentation: 50% chance to create augmented version\n",
        "        if np.random.random() > 0.5:\n",
        "            # Add noise to original audio\n",
        "            noisy_audio = add_noise(audio)\n",
        "\n",
        "            # Extract features from noisy audio\n",
        "            try:\n",
        "                # Create temporary features for noisy audio\n",
        "                temp_features = {}\n",
        "                temp_features['mfcc'] = librosa.feature.mfcc(y=noisy_audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC)\n",
        "                temp_features['mel_spectrogram'] = librosa.power_to_db(\n",
        "                    librosa.feature.melspectrogram(y=noisy_audio, sr=SAMPLE_RATE, n_mels=N_MELS),\n",
        "                    ref=np.max\n",
        "                )\n",
        "                temp_features['chroma'] = librosa.feature.chroma_stft(y=noisy_audio, sr=SAMPLE_RATE)\n",
        "                temp_features['spectral_contrast'] = librosa.feature.spectral_contrast(y=noisy_audio, sr=SAMPLE_RATE)\n",
        "                temp_features['zcr'] = librosa.feature.zero_crossing_rate(noisy_audio)\n",
        "\n",
        "                noisy_lstm_features = prepare_lstm_features(temp_features)\n",
        "\n",
        "                # Add augmented sample\n",
        "                X_features.append(noisy_lstm_features)\n",
        "                X_raw_audio.append(noisy_audio)\n",
        "                y_labels.append(row['emotion'])\n",
        "                file_info.append(row)\n",
        "                all_features_for_viz.append(temp_features)\n",
        "                augmented_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è Error in augmentation for file {idx}: {e}\")\n",
        "\n",
        "    # Progress update every 50 files or at the end\n",
        "    if (idx + 1) % 50 == 0 or idx == len(file_df) - 1:\n",
        "        print(f\"   Progress: {idx + 1}/{len(file_df)} files processed\")\n",
        "\n",
        "print(f\"‚úÖ Feature extraction complete!\")\n",
        "print(f\"   Original samples processed: {processed_count}\")\n",
        "print(f\"   Augmented samples created: {augmented_count}\")\n",
        "print(f\"   Total samples: {len(X_features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8fVy18BWWqF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 9: Feature Visualization Examples =========================\n",
        "print(\"\\nüé® STEP 9: Visualizing Extracted Features\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Let's visualize the features we extracted to understand what they represent.\")\n",
        "\n",
        "# Show detailed features for first sample of each emotion\n",
        "for emotion_label, indices in viz_samples.items():\n",
        "    if indices:  # If we have samples for this emotion\n",
        "        idx = indices[0]  # Take the first sample\n",
        "        row_info = file_info[idx]\n",
        "        features = all_features_for_viz[idx]\n",
        "        audio = X_raw_audio[idx]\n",
        "\n",
        "        print(f\"\\nüìà Showing feature visualization for '{emotion_label.upper()}' emotion:\")\n",
        "        print(f\"   File: {row_info['filename']}\")\n",
        "        print(f\"   Actor: {row_info['actor']} ({row_info['gender']})\")\n",
        "        print(f\"   Intensity: {'Normal' if row_info['intensity'] == 1 else 'Strong'}\")\n",
        "\n",
        "        visualize_audio_features(features, audio, emotion_label, row_info['filename'])\n",
        "\n",
        "# Compare features across different emotions\n",
        "print(\"\\nüîç Comparing Features Across Different Emotions:\")\n",
        "print(\"This helps us see how different emotions create different patterns.\")\n",
        "\n",
        "comparison_features = []\n",
        "comparison_labels = []\n",
        "comparison_indices = []\n",
        "\n",
        "# Select up to 4 emotions for comparison\n",
        "for emotion_label, indices in list(viz_samples.items())[:4]:\n",
        "    if indices:\n",
        "        idx = indices[0]\n",
        "        comparison_features.append(all_features_for_viz[idx])\n",
        "        comparison_labels.append(emotion_label)\n",
        "        comparison_indices.append(idx)\n",
        "\n",
        "if comparison_features:\n",
        "    compare_emotions_features(comparison_features, comparison_labels, comparison_indices)\n",
        "\n",
        "# Show feature dimension statistics\n",
        "print(f\"\\nüìä Feature Dimension Summary:\")\n",
        "if all_features_for_viz:\n",
        "    sample_features = all_features_for_viz[0]\n",
        "    print(f\"   MFCC: {sample_features['mfcc'].shape} (coefficients x time_frames)\")\n",
        "    print(f\"   Mel-spectrogram: {sample_features['mel_spectrogram'].shape} (mel_bands x time_frames)\")\n",
        "    print(f\"   Chroma: {sample_features['chroma'].shape} (pitch_classes x time_frames)\")\n",
        "    print(f\"   Spectral contrast: {sample_features['spectral_contrast'].shape} (bands x time_frames)\")\n",
        "    print(f\"   Zero crossing rate: {sample_features['zcr'].shape} (1 x time_frames)\")\n",
        "\n",
        "# Emotion distribution visualization\n",
        "print(\"\\nüìà Dataset Distribution Analysis:\")\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Pie chart of emotion distribution\n",
        "plt.subplot(1, 3, 1)\n",
        "emotion_counts = pd.Series([file_info[i]['emotion_label'] for i in range(len(file_info))]).value_counts()\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(emotion_counts)))\n",
        "plt.pie(emotion_counts.values, labels=emotion_counts.index, autopct='%1.1f%%', colors=colors)\n",
        "plt.title('Emotion Distribution')\n",
        "\n",
        "# Bar chart of emotion counts\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(emotion_counts.index, emotion_counts.values, color=colors)\n",
        "plt.title('Sample Count per Emotion')\n",
        "plt.xlabel('Emotion Category')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Gender distribution\n",
        "plt.subplot(1, 3, 3)\n",
        "gender_counts = pd.Series([file_info[i]['gender'] for i in range(len(file_info))]).value_counts()\n",
        "plt.bar(gender_counts.index, gender_counts.values, color=['lightblue', 'lightpink'])\n",
        "plt.title('Gender Distribution')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Number of Samples')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_C8oh46WbcE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 10: Preparing Data for LSTM =========================\n",
        "print(\"\\nüîß STEP 10: Preparing Data for LSTM Neural Network\")\n",
        "print(\"-\" * 50)\n",
        "print(\"LSTM (Long Short-Term Memory) networks work with sequences.\")\n",
        "print(\"We need to format our data properly for the LSTM to understand it.\")\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_length = max([x.shape[0] for x in X_features])\n",
        "print(f\"üìè Maximum sequence length found: {max_length} time frames\")\n",
        "print(f\"   This represents {max_length * (DURATION / max_length):.2f} seconds of audio\")\n",
        "\n",
        "def pad_sequence(seq, max_len):\n",
        "    \"\"\"\n",
        "    Pad sequences to have the same length.\n",
        "    Shorter sequences are padded with zeros.\n",
        "    Longer sequences are truncated.\n",
        "\n",
        "    Args:\n",
        "        seq (np.array): Input sequence\n",
        "        max_len (int): Target length\n",
        "\n",
        "    Returns:\n",
        "        np.array: Padded/truncated sequence\n",
        "    \"\"\"\n",
        "    if seq.shape[0] < max_len:\n",
        "        # Pad with zeros if too short\n",
        "        pad_width = max_len - seq.shape[0]\n",
        "        return np.pad(seq, ((0, pad_width), (0, 0)), mode='constant')\n",
        "    else:\n",
        "        # Truncate if too long\n",
        "        return seq[:max_len]\n",
        "\n",
        "print(\"‚öôÔ∏è Padding all sequences to uniform length...\")\n",
        "# Pad all sequences to the same length\n",
        "X_padded = np.array([pad_sequence(x, max_length) for x in X_features])\n",
        "y_array = np.array(y_labels)\n",
        "\n",
        "print(f\"‚úÖ Feature matrix shape: {X_padded.shape}\")\n",
        "print(f\"   Format: (samples, time_steps, features)\")\n",
        "print(f\"   - {X_padded.shape[0]} samples\")\n",
        "print(f\"   - {X_padded.shape[1]} time steps\")\n",
        "print(f\"   - {X_padded.shape[2]} features (mel frequency bands)\")\n",
        "print(f\"üìä Label array shape: {y_array.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8N5JwE5Wkxa"
      },
      "outputs": [],
      "source": [
        "# ========================= STEP 11: Dataset Splitting =========================\n",
        "print(\"\\nüìÇ STEP 11: Splitting Dataset into Train/Validation/Test Sets\")\n",
        "print(\"-\" * 50)\n",
        "print(\"We split our data into three parts:\")\n",
        "print(\"‚Ä¢ Training set (70%): Used to train the model\")\n",
        "print(\"‚Ä¢ Validation set (15%): Used to tune the model during training\")\n",
        "print(\"‚Ä¢ Test set (15%): Used to evaluate final model performance\")\n",
        "\n",
        "# First split: separate test set (15%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_padded, y_array,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=y_array  # Ensure balanced distribution\n",
        ")\n",
        "\n",
        "# Second split: separate training and validation (from remaining 85%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    test_size=0.176,  # 0.176 * 0.85 ‚âà 0.15 of total data\n",
        "    random_state=42,\n",
        "    stratify=y_train_val\n",
        ")\n",
        "\n",
        "print(f\"üìä Dataset split complete:\")\n",
        "print(f\"   Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_padded)*100:.1f}%)\")\n",
        "print(f\"   Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X_padded)*100:.1f}%)\")\n",
        "print(f\"   Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_padded)*100:.1f}%)\")\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "print(\"\\nüî¢ Converting labels to one-hot encoding...\")\n",
        "print(\"One-hot encoding converts labels like 'happy' to vectors like [0,0,1,0,0,0,0,0]\")\n",
        "\n",
        "num_classes = len(EMOTIONS)\n",
        "y_train_cat = to_categorical(y_train, num_classes)\n",
        "y_val_cat = to_categorical(y_val, num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f\"‚úÖ One-hot encoding complete:\")\n",
        "print(f\"   Number of emotion classes: {num_classes}\")\n",
        "print(f\"   Original label example: {y_train[0]} ({EMOTIONS[y_train[0]]})\")\n",
        "print(f\"   One-hot label example: {y_train_cat[0]}\")\n",
        "\n",
        "# Check class distribution in each set\n",
        "print(f\"\\nüìà Class distribution verification:\")\n",
        "for set_name, y_set in [(\"Training\", y_train), (\"Validation\", y_val), (\"Test\", y_test)]:\n",
        "    unique, counts = np.unique(y_set, return_counts=True)\n",
        "    print(f\"   {set_name} set:\")\n",
        "    for emotion_id, count in zip(unique, counts):\n",
        "        emotion_name = EMOTIONS[emotion_id]\n",
        "        percentage = count / len(y_set) * 100\n",
        "        print(f\"     {emotion_name}: {count} samples ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4qvGa_dWmnk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 12: Building the LSTM Model =========================\n",
        "print(\"\\nüß† STEP 12: Building the LSTM Neural Network\")\n",
        "print(\"-\" * 50)\n",
        "print(\"LSTM (Long Short-Term Memory) networks are perfect for sequence data like audio.\")\n",
        "print(\"They can remember patterns across time, which is important for emotion recognition.\")\n",
        "print(\"\\nOur model architecture:\")\n",
        "print(\"1. LSTM Layer 1: 128 units, returns sequences\")\n",
        "print(\"2. Batch Normalization: Stabilizes training\")\n",
        "print(\"3. Dropout: Prevents overfitting (30%)\")\n",
        "print(\"4. LSTM Layer 2: 64 units, final output\")\n",
        "print(\"5. Batch Normalization: Stabilizes training\")\n",
        "print(\"6. Dropout: Prevents overfitting (30%)\")\n",
        "print(\"7. Dense Layer 1: 64 neurons, ReLU activation\")\n",
        "print(\"8. Dropout: Prevents overfitting (50%)\")\n",
        "print(\"9. Dense Layer 2: 32 neurons, ReLU activation\")\n",
        "print(\"10. Output Layer: 8 neurons (one per emotion), Softmax activation\")\n",
        "\n",
        "def create_emotion_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Create an LSTM model for emotion recognition.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of input data (time_steps, features)\n",
        "        num_classes (int): Number of emotion classes\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.keras.Model: Compiled LSTM model\n",
        "    \"\"\"\n",
        "    print(f\"üèóÔ∏è Building model with input shape: {input_shape}\")\n",
        "    print(f\"   Output classes: {num_classes}\")\n",
        "\n",
        "    model = Sequential([\n",
        "        # First LSTM layer - processes sequences and passes to next layer\n",
        "        LSTM(128,\n",
        "             return_sequences=True,  # Return full sequence for next LSTM layer\n",
        "             input_shape=input_shape,\n",
        "             name='lstm_1'),\n",
        "        BatchNormalization(name='batch_norm_1'),\n",
        "        Dropout(0.3, name='dropout_1'),\n",
        "\n",
        "        # Second LSTM layer - final sequence processing\n",
        "        LSTM(64,\n",
        "             return_sequences=False,  # Return only final output\n",
        "             name='lstm_2'),\n",
        "        BatchNormalization(name='batch_norm_2'),\n",
        "        Dropout(0.3, name='dropout_2'),\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        Dense(64, activation='relu', name='dense_1'),\n",
        "        BatchNormalization(name='batch_norm_3'),\n",
        "        Dropout(0.5, name='dropout_3'),\n",
        "\n",
        "        Dense(32, activation='relu', name='dense_2'),\n",
        "        Dropout(0.3, name='dropout_4'),\n",
        "\n",
        "        # Output layer - one neuron per emotion class\n",
        "        Dense(num_classes, activation='softmax', name='output')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])  # (time_steps, features)\n",
        "model = create_emotion_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "print(\"\\n‚öôÔ∏è Compiling the model...\")\n",
        "print(\"‚Ä¢ Optimizer: Adam (adaptive learning rate)\")\n",
        "print(\"‚Ä¢ Loss function: Categorical crossentropy (for multi-class classification)\")\n",
        "print(\"‚Ä¢ Metrics: Accuracy\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display model architecture\n",
        "print(\"\\nüèóÔ∏è Model Architecture Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Calculate total parameters\n",
        "total_params = model.count_params()\n",
        "print(f\"\\nüìä Model Statistics:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Model layers: {len(model.layers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsefTmLEWsn3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 13: Setting Up Training Callbacks =========================\n",
        "print(\"\\n‚öôÔ∏è STEP 13: Setting Up Training Callbacks\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Callbacks help us train the model more effectively:\")\n",
        "print(\"‚Ä¢ Early Stopping: Stops training if validation accuracy doesn't improve\")\n",
        "print(\"‚Ä¢ Learning Rate Reduction: Reduces learning rate when training plateaus\")\n",
        "\n",
        "# Early stopping: prevent overfitting\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',      # Watch validation accuracy\n",
        "    patience=10,                 # Wait 10 epochs before stopping\n",
        "    restore_best_weights=True,   # Use the best weights found\n",
        "    verbose=1,                   # Print when stopping\n",
        "    mode='max'                   # We want to maximize accuracy\n",
        ")\n",
        "\n",
        "# Learning rate reduction: improve convergence\n",
        "lr_reduction = ReduceLROnPlateau(\n",
        "    monitor='val_loss',          # Watch validation loss\n",
        "    factor=0.5,                  # Reduce LR by half\n",
        "    patience=5,                  # Wait 5 epochs before reducing\n",
        "    min_lr=1e-7,                # Don't go below this learning rate\n",
        "    verbose=1                    # Print when reducing\n",
        ")\n",
        "\n",
        "callbacks = [early_stopping, lr_reduction]\n",
        "\n",
        "print(\"‚úÖ Callbacks configured:\")\n",
        "print(f\"   Early stopping patience: 10 epochs\")\n",
        "print(f\"   Learning rate reduction factor: 0.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIMEEVUVW0Fj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 14: Training the Model =========================\n",
        "print(\"\\nüöÄ STEP 14: Training the Model\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Now we train our LSTM model on the emotion recognition task.\")\n",
        "print(\"This process will take several minutes...\")\n",
        "print(f\"\\nTraining parameters:\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Maximum epochs: {EPOCHS}\")\n",
        "print(f\"   Training samples: {len(X_train)}\")\n",
        "print(f\"   Validation samples: {len(X_val)}\")\n",
        "\n",
        "print(\"\\nüèÉ‚Äç‚ôÇÔ∏è Starting training...\")\n",
        "print(\"Watch the accuracy and loss values:\")\n",
        "print(\"‚Ä¢ Training accuracy should increase over time\")\n",
        "print(\"‚Ä¢ Validation accuracy should also increase (and not lag behind too much)\")\n",
        "print(\"‚Ä¢ Training loss should decrease over time\")\n",
        "print(\"‚Ä¢ Validation loss should also decrease\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train_cat,                    # Training data\n",
        "    validation_data=(X_val, y_val_cat),     # Validation data\n",
        "    epochs=EPOCHS,                          # Maximum number of epochs\n",
        "    batch_size=BATCH_SIZE,                  # Samples per batch\n",
        "    callbacks=callbacks,                    # Early stopping and LR reduction\n",
        "    verbose=1                               # Show progress bar\n",
        ")\n",
        "\n",
        "print(\"\\nüéâ Training completed!\")\n",
        "\n",
        "# Get training history for analysis\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "print(f\"üìä Training Results:\")\n",
        "print(f\"   Final training accuracy: {train_acc[-1]:.4f} ({train_acc[-1]*100:.1f}%)\")\n",
        "print(f\"   Final validation accuracy: {val_acc[-1]:.4f} ({val_acc[-1]*100:.1f}%)\")\n",
        "print(f\"   Best validation accuracy: {max(val_acc):.4f} ({max(val_acc)*100:.1f}%)\")\n",
        "print(f\"   Total epochs trained: {len(train_acc)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFiMMkxIW3kw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 15: Model Evaluation =========================\n",
        "print(\"\\nüìä STEP 15: Evaluating Model Performance\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Now let's see how well our model performs on the test set.\")\n",
        "print(\"The test set contains data the model has never seen before.\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"üß™ Testing model on unseen data...\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "\n",
        "print(f\"\\nüéØ Test Results:\")\n",
        "print(f\"   Test Loss: {test_loss:.4f}\")\n",
        "print(f\"   Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)\")\n",
        "\n",
        "# Generate predictions\n",
        "print(\"\\nüîÆ Generating detailed predictions...\")\n",
        "y_pred = model.predict(X_test, verbose=0)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate additional metrics\n",
        "print(\"\\nüìà Detailed Performance Analysis:\")\n",
        "\n",
        "# Classification report\n",
        "emotion_names = [EMOTIONS[i] for i in range(num_classes)]\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(\"This shows precision, recall, and F1-score for each emotion.\")\n",
        "print(\"‚Ä¢ Precision: Of all predictions for this emotion, how many were correct?\")\n",
        "print(\"‚Ä¢ Recall: Of all actual instances of this emotion, how many were found?\")\n",
        "print(\"‚Ä¢ F1-score: Harmonic mean of precision and recall\")\n",
        "print(\"\\n\" + classification_report(y_test, y_pred_classes, target_names=emotion_names))\n",
        "\n",
        "# Confusion matrix analysis\n",
        "print(\"\\nüîç Confusion Matrix Analysis:\")\n",
        "print(\"Shows which emotions the model confuses with each other.\")\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "# Print simplified confusion matrix with emotion names\n",
        "print(\"\\nConfusion Matrix (Actual vs Predicted):\")\n",
        "print(f\"{'':12}\", end=\"\")\n",
        "for emotion in emotion_names:\n",
        "    print(f\"{emotion[:8]:>8}\", end=\"\")\n",
        "print()\n",
        "\n",
        "for i, actual_emotion in enumerate(emotion_names):\n",
        "    print(f\"{actual_emotion[:12]:12}\", end=\"\")\n",
        "    for j in range(len(emotion_names)):\n",
        "        print(f\"{cm[i,j]:8d}\", end=\"\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPQ8CiR6W7MI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 16: Visualization of Results =========================\n",
        "print(\"\\nüìà STEP 16: Visualizing Training Results and Performance\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "# Training history - Accuracy\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(train_acc, label='Training Accuracy', linewidth=2, color='blue')\n",
        "plt.plot(val_acc, label='Validation Accuracy', linewidth=2, color='orange')\n",
        "plt.title('Model Accuracy Over Time', fontsize=14)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Training history - Loss\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(train_loss, label='Training Loss', linewidth=2, color='red')\n",
        "plt.plot(val_loss, label='Validation Loss', linewidth=2, color='purple')\n",
        "plt.title('Model Loss Over Time', fontsize=14)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion Matrix Heatmap\n",
        "plt.subplot(2, 3, 3)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=[name[:6] for name in emotion_names],\n",
        "           yticklabels=[name[:6] for name in emotion_names])\n",
        "plt.title('Confusion Matrix', fontsize=14)\n",
        "plt.xlabel('Predicted Emotion')\n",
        "plt.ylabel('Actual Emotion')\n",
        "\n",
        "# Per-class accuracy\n",
        "plt.subplot(2, 3, 4)\n",
        "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "bars = plt.bar(range(len(emotion_names)), class_accuracy,\n",
        "               color=plt.cm.viridis(np.linspace(0, 1, len(emotion_names))))\n",
        "plt.title('Accuracy per Emotion Class', fontsize=14)\n",
        "plt.xlabel('Emotion')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(range(len(emotion_names)), [name[:6] for name in emotion_names], rotation=45)\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add accuracy values on bars\n",
        "for i, (bar, acc) in enumerate(zip(bars, class_accuracy)):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{acc:.2f}', ha='center', va='bottom')\n",
        "\n",
        "# Model prediction confidence distribution\n",
        "plt.subplot(2, 3, 5)\n",
        "confidence_scores = np.max(y_pred, axis=1)\n",
        "plt.hist(confidence_scores, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
        "plt.title('Prediction Confidence Distribution', fontsize=14)\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Number of Predictions')\n",
        "plt.axvline(np.mean(confidence_scores), color='red', linestyle='--',\n",
        "           label=f'Mean: {np.mean(confidence_scores):.3f}')\n",
        "plt.legend()\n",
        "\n",
        "# Learning curve comparison\n",
        "plt.subplot(2, 3, 6)\n",
        "epochs = range(1, len(train_acc) + 1)\n",
        "plt.plot(epochs, train_acc, 'b-', label='Training Accuracy', linewidth=2)\n",
        "plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy', linewidth=2)\n",
        "plt.fill_between(epochs, train_acc, alpha=0.3, color='blue')\n",
        "plt.fill_between(epochs, val_acc, alpha=0.3, color='red')\n",
        "plt.title('Learning Curves Comparison', fontsize=14)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\nüìä Final Performance Summary:\")\n",
        "print(f\"   Best training accuracy: {max(train_acc):.4f}\")\n",
        "print(f\"   Best validation accuracy: {max(val_acc):.4f}\")\n",
        "print(f\"   Final test accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"   Average prediction confidence: {np.mean(confidence_scores):.3f}\")\n",
        "print(f\"   Model converged in: {len(train_acc)} epochs\")\n",
        "\n",
        "# Identify best and worst performing emotions\n",
        "best_emotion_idx = np.argmax(class_accuracy)\n",
        "worst_emotion_idx = np.argmin(class_accuracy)\n",
        "print(f\"   Best recognized emotion: {emotion_names[best_emotion_idx]} ({class_accuracy[best_emotion_idx]:.3f})\")\n",
        "print(f\"   Most challenging emotion: {emotion_names[worst_emotion_idx]} ({class_accuracy[worst_emotion_idx]:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKB9I5m9W_lI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 17: Saving the Model =========================\n",
        "print(\"\\nüíæ STEP 17: Saving the Trained Model\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Saving our trained model so we can use it later without retraining.\")\n",
        "\n",
        "# Save the model\n",
        "model.save('/content/emotion_recognition_model.h5')\n",
        "print(\"‚úÖ Model saved as 'emotion_recognition_model.h5'\")\n",
        "\n",
        "# Save emotion labels mapping\n",
        "import json\n",
        "with open('/content/emotion_labels.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(EMOTIONS, f, ensure_ascii=False, indent=2)\n",
        "print(\"‚úÖ Emotion labels saved as 'emotion_labels.json'\")\n",
        "\n",
        "# Save training history\n",
        "history_dict = {\n",
        "    'train_accuracy': train_acc,\n",
        "    'val_accuracy': val_acc,\n",
        "    'train_loss': train_loss,\n",
        "    'val_loss': val_loss,\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'test_loss': float(test_loss)\n",
        "}\n",
        "\n",
        "with open('/content/training_history.json', 'w') as f:\n",
        "    json.dump(history_dict, f, indent=2)\n",
        "print(\"‚úÖ Training history saved as 'training_history.json'\")\n",
        "\n",
        "print(f\"\\nüìÅ All files saved to /content/:\")\n",
        "print(f\"   ‚Ä¢ emotion_recognition_model.h5 (trained model)\")\n",
        "print(f\"   ‚Ä¢ emotion_labels.json (emotion mappings)\")\n",
        "print(f\"   ‚Ä¢ training_history.json (training metrics)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZvCPmPBXC6y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 18: Testing Individual Predictions =========================\n",
        "print(\"\\nüß™ STEP 18: Testing Individual Audio File Predictions\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Let's test our model on individual audio files to see how it performs.\")\n",
        "\n",
        "def predict_emotion(file_path, model, emotions_dict):\n",
        "    \"\"\"\n",
        "    Predict emotion for a single audio file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to audio file\n",
        "        model: Trained Keras model\n",
        "        emotions_dict (dict): Emotion ID to name mapping\n",
        "\n",
        "    Returns:\n",
        "        tuple: (predicted_emotion_name, confidence_score, all_probabilities)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract features\n",
        "        features, audio = extract_audio_features(file_path)\n",
        "        if features is None:\n",
        "            return None, None, None\n",
        "\n",
        "        # Prepare for LSTM\n",
        "        lstm_features = prepare_lstm_features(features)\n",
        "        lstm_features_padded = pad_sequence(lstm_features, max_length)\n",
        "        lstm_features_batch = np.expand_dims(lstm_features_padded, axis=0)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(lstm_features_batch, verbose=0)\n",
        "\n",
        "        # Get results\n",
        "        emotion_id = np.argmax(prediction)\n",
        "        confidence = np.max(prediction)\n",
        "        emotion_name = emotions_dict[emotion_id]\n",
        "\n",
        "        return emotion_name, confidence, prediction[0]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error predicting emotion: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def analyze_prediction(prediction_probs, emotions_dict):\n",
        "    \"\"\"\n",
        "    Analyze and display prediction probabilities.\n",
        "\n",
        "    Args:\n",
        "        prediction_probs (np.array): Probability for each emotion class\n",
        "        emotions_dict (dict): Emotion ID to name mapping\n",
        "    \"\"\"\n",
        "    print(\"   Prediction probabilities:\")\n",
        "    for i, prob in enumerate(prediction_probs):\n",
        "        emotion_name = emotions_dict[i]\n",
        "        print(f\"     {emotion_name:10}: {prob:.3f} ({prob*100:.1f}%)\")\n",
        "\n",
        "# Test on several example files\n",
        "print(\"üß™ Testing model predictions on sample files:\")\n",
        "test_indices = np.random.choice(len(file_df), size=min(5, len(file_df)), replace=False)\n",
        "\n",
        "for i, idx in enumerate(test_indices):\n",
        "    test_file = file_df.iloc[idx]\n",
        "    print(f\"\\nüìÑ Test Example {i+1}:\")\n",
        "    print(f\"   File: {test_file['filename']}\")\n",
        "    print(f\"   Actual emotion: {test_file['emotion_label'].upper()}\")\n",
        "    print(f\"   Actor: {test_file['actor']} ({test_file['gender']})\")\n",
        "    print(f\"   Intensity: {'Normal' if test_file['intensity'] == 1 else 'Strong'}\")\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_emotion, confidence, all_probs = predict_emotion(\n",
        "        test_file['file_path'], model, EMOTIONS\n",
        "    )\n",
        "\n",
        "    if predicted_emotion:\n",
        "        print(f\"   Predicted emotion: {predicted_emotion.upper()}\")\n",
        "        print(f\"   Confidence: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
        "\n",
        "        # Check if prediction is correct\n",
        "        is_correct = predicted_emotion == test_file['emotion_label']\n",
        "        print(f\"   Result: {'‚úÖ CORRECT' if is_correct else '‚ùå INCORRECT'}\")\n",
        "\n",
        "        # Show detailed probabilities\n",
        "        analyze_prediction(all_probs, EMOTIONS)\n",
        "    else:\n",
        "        print(\"   ‚ùå Failed to make prediction\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgNiiw9NXHE7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 19: Understanding Model Behavior =========================\n",
        "print(\"\\nüî¨ STEP 19: Understanding What the Model Learned\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Let's analyze some patterns in our model's behavior.\")\n",
        "\n",
        "# Analyze common misclassifications\n",
        "print(\"üîç Analyzing common misclassifications:\")\n",
        "misclassified_pairs = {}\n",
        "\n",
        "for actual, predicted in zip(y_test, y_pred_classes):\n",
        "    if actual != predicted:\n",
        "        pair = (EMOTIONS[actual], EMOTIONS[predicted])\n",
        "        misclassified_pairs[pair] = misclassified_pairs.get(pair, 0) + 1\n",
        "\n",
        "# Show top 5 most common misclassifications\n",
        "if misclassified_pairs:\n",
        "    sorted_pairs = sorted(misclassified_pairs.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"\\nMost common misclassifications:\")\n",
        "    for i, ((actual, predicted), count) in enumerate(sorted_pairs[:5]):\n",
        "        print(f\"   {i+1}. {actual} ‚Üí {predicted}: {count} times\")\n",
        "\n",
        "    print(\"\\nüí° Insights:\")\n",
        "    print(\"   ‚Ä¢ Look for emotions that are commonly confused\")\n",
        "    print(\"   ‚Ä¢ Similar emotions (like 'calm' and 'neutral') might be hard to distinguish\")\n",
        "    print(\"   ‚Ä¢ This helps us understand model limitations\")\n",
        "\n",
        "# Analyze confidence by emotion\n",
        "print(f\"\\nüìä Confidence analysis by emotion:\")\n",
        "confidence_by_emotion = {}\n",
        "for actual, pred_prob in zip(y_test, y_pred):\n",
        "    emotion_name = EMOTIONS[actual]\n",
        "    confidence = np.max(pred_prob)\n",
        "    if emotion_name not in confidence_by_emotion:\n",
        "        confidence_by_emotion[emotion_name] = []\n",
        "    confidence_by_emotion[emotion_name].append(confidence)\n",
        "\n",
        "print(\"Average confidence per emotion:\")\n",
        "for emotion, confidences in confidence_by_emotion.items():\n",
        "    avg_confidence = np.mean(confidences)\n",
        "    print(f\"   {emotion:10}: {avg_confidence:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhSVLkZxXKZR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= STEP 20: Conclusion and Next Steps =========================\n",
        "print(\"\\nüéì STEP 20: Tutorial Conclusion and Next Steps\")\n",
        "print(\"=\" * 60)\n",
        "print(\"üéâ Congratulations! You have successfully built an emotion recognition system!\")\n",
        "print(\"\\nüìö What you learned:\")\n",
        "print(\"   ‚úÖ Audio feature extraction (MFCC, Mel-spectrogram, Chroma, etc.)\")\n",
        "print(\"   ‚úÖ Data preprocessing and augmentation techniques\")\n",
        "print(\"   ‚úÖ LSTM neural network architecture for sequence classification\")\n",
        "print(\"   ‚úÖ Model training with callbacks and optimization\")\n",
        "print(\"   ‚úÖ Performance evaluation and visualization\")\n",
        "print(\"   ‚úÖ Individual prediction and model interpretation\")\n",
        "\n",
        "print(f\"\\nüìä Your model's final performance:\")\n",
        "print(f\"   üéØ Test Accuracy: {test_accuracy:.1%}\")\n",
        "print(f\"   üìà Best Validation Accuracy: {max(val_acc):.1%}\")\n",
        "print(f\"   üß† Total Parameters: {total_params:,}\")\n",
        "print(f\"   ‚è±Ô∏è Training Epochs: {len(train_acc)}\")\n",
        "\n",
        "print(f\"\\nüöÄ Next steps to improve your model:\")\n",
        "print(\"   1. üéµ Try different audio features (spectral rolloff, tonnetz, etc.)\")\n",
        "print(\"   2. üèóÔ∏è Experiment with different architectures (CNN+LSTM, Transformers)\")\n",
        "print(\"   3. üìä Use more data augmentation techniques\")\n",
        "print(\"   4. ‚öôÔ∏è Tune hyperparameters (learning rate, batch size, dropout)\")\n",
        "print(\"   5. üé≠ Add more emotion classes or intensity levels\")\n",
        "print(\"   6. üîÑ Try transfer learning with pre-trained models\")\n",
        "print(\"   7. üì± Deploy your model to a web app or mobile app\")\n",
        "\n",
        "print(f\"\\nüîß How to use your saved model:\")\n",
        "print(\"   ```python\")\n",
        "print(\"   from tensorflow.keras.models import load_model\")\n",
        "print(\"   model = load_model('emotion_recognition_model.h5')\")\n",
        "print(\"   emotion, confidence, _ = predict_emotion('new_audio.wav', model, EMOTIONS)\")\n",
        "print(\"   ```\")\n",
        "\n",
        "print(f\"\\nüåü Applications of emotion recognition:\")\n",
        "print(\"   ‚Ä¢ üé≠ Interactive entertainment systems\")\n",
        "print(\"   ‚Ä¢ üè• Mental health monitoring\")\n",
        "print(\"   ‚Ä¢ üìû Customer service analysis\")\n",
        "print(\"   ‚Ä¢ ü§ñ Human-computer interaction\")\n",
        "print(\"   ‚Ä¢ üìö Educational technology\")\n",
        "print(\"   ‚Ä¢ üé¨ Content recommendation systems\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéä Thank you for completing the RAVDESS Emotion Recognition Tutorial!\")\n",
        "print(\"Keep experimenting and learning! üöÄ\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}