{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2cc2b05b",
      "metadata": {
        "id": "2cc2b05b"
      },
      "source": [
        "# Physics-Informed Neural Networks for Burgers Equation\n",
        "## A Tutorial on Implementing Viscous and Inviscid Solutions\n",
        "\n",
        "In this tutorial, we'll explore Physics-Informed Neural Networks (PINNs) by implementing solvers for both the viscous and inviscid Burgers equation. The tutorial will guide you through:\n",
        "\n",
        "1. Setting up a PINN architecture\n",
        "2. Implementing the viscous Burgers equation (provided)\n",
        "3. Implementing the inviscid Burgers equation (exercise)\n",
        "4. Comparing and analyzing the results\n",
        "\n",
        "The Burgers equation is a fundamental PDE that appears in various fields of fluid mechanics and acoustics. We'll study both versions:\n",
        "\n",
        "- Viscous: $\\frac{\\partial u}{\\partial t} + u\\frac{\\partial u}{\\partial x} = \\nu\\frac{\\partial^2 u}{\\partial x^2}$\n",
        "- Inviscid: $\\frac{\\partial u}{\\partial t} + u\\frac{\\partial u}{\\partial x} = 0$\n",
        "\n",
        "where $\\nu$ is the viscosity coefficient."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e7aa57",
      "metadata": {
        "id": "f7e7aa57"
      },
      "source": [
        "## 1. Introduction to vanilla PINNs <a name=\"1-bullet\"></a>\n",
        "\n",
        "First, let us recall the general formulation of the vanilla PINNs. Considering the following parameterized PDE defined on the domain $\\Omega \\subset \\mathbb{R}^d$ with the boundary $\\partial\\Omega$:\n",
        "\\begin{align}\n",
        "&\\partial_t \\boldsymbol{u} + N_{\\boldsymbol{\\mathrm{x}}}(\\boldsymbol{u}, \\lambda)=0, \\text{ for } \\boldsymbol{\\mathrm{x}}\\in\\Omega, t\\in[0,T]\\\\\n",
        "&\\boldsymbol{u}(\\boldsymbol{\\mathrm{x}},0)=g(\\boldsymbol{\\mathrm{x}}), \\text{ for } \\boldsymbol{\\mathrm{x}}\\in\\Omega\\\\\n",
        "&B(\\boldsymbol{u},\\boldsymbol{\\mathrm{x}},t)=0, \\text{ for } \\boldsymbol{\\mathrm{x}}\\in\\partial\\Omega\\\\\n",
        "\\end{align}\n",
        "where $\\boldsymbol{\\mathrm{x}} \\in \\mathbb{R}^d$ and $t$ are the spatial and temporal coordinates, $N_{\\boldsymbol{\\mathrm{x}}}$ is a differential operator, $\\boldsymbol{\\lambda}$ is the PDE parameter, $\\boldsymbol{u}$ is the solution of the PDE with initial condition $g(\\boldsymbol{\\mathrm{x}})$ and boundary condition $B$, which could be Dirichlet, Neumann, Robin, or periodic boundary conditions. The subscripts denote the partial differentiation in time or space. In PINNs, the solution $\\boldsymbol{u}$ of the PDE is approximated by a fully-connected feedforward neural network $\\Phi$:\n",
        "\\begin{align*}\n",
        "    \\boldsymbol{u} \\approx \\hat{\\boldsymbol{u}} = \\Phi(\\boldsymbol{\\mathrm{x}}, t, \\boldsymbol{\\theta})\n",
        "\\end{align*}\n",
        "where $\\hat{\\boldsymbol{u}}$ denotes the prediction value for the solution and $\\boldsymbol{\\theta}$ denotes the trainable parameters of the neural network. The parameters of the neural network are trained by minimizing the cost function $L$:\n",
        "\\begin{align}\n",
        "    L = L_{pde} + L_{ic} + L_{bc} + L_{data}\n",
        "\\end{align}\n",
        "where the terms $L_{pde},L_{ic},L_{bc}$ and $L_{data}$ penalize the loss in the residual of the PDE, the initial condition, the boundary condition, and the supervised data (measurements), respectively, which can be represented as follow:\n",
        "\\begin{align*}\n",
        "    L_{pde} &= \\dfrac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}|\\hat{\\boldsymbol{u}}_{t^i} + \\Phi_{\\boldsymbol{\\mathrm{x}}^{i}}(\\hat{\\boldsymbol{u}^{i}}, \\boldsymbol{\\lambda})|^2\\\\\n",
        "    L_{ic} &= \\dfrac{w_{ic}}{N_{ic}}\\sum_{i=1}^{N_{ic}}|\\boldsymbol{\\hat{u}}(\\boldsymbol{\\mathrm{x}}^i,0) - g(\\boldsymbol{\\mathrm{x}}^i)|^2\\\\\n",
        "    L_{bc} &= \\dfrac{w_{bc}}{N_{bc}}\\sum_{i=1}^{N_{bc}}|B(\\hat{\\boldsymbol{u}^i}, \\boldsymbol{\\mathrm{x}}^{i}, t^{i})|^2\\\\\n",
        "    L_{data} &= \\dfrac{w_{data}}{N_{data}}\\sum_{i=1}^{N_{data}}|\\hat{\\boldsymbol{u}}(\\boldsymbol{\\mathrm{x}}^i,t^i) - \\boldsymbol{u}(\\boldsymbol{\\mathrm{x}}^i,t^i)|^2\\\\\n",
        "\\end{align*}\n",
        "where $w_{ic}, w_{bc}, w_{data}$ are the weights to balance different terms in the cost function.\n",
        "\n",
        "The definition of the cost function $L$ and the trainable parameters depend on the problems:\n",
        "\n",
        "|Type of problem | Input of $\\Phi$ | Trainable parameter | Loss function |\n",
        "| :- | -: | -: |-: |\n",
        "| Forward | $(\\boldsymbol{\\mathrm{x}},t)$ | $\\theta$ | $L = L_{pde} + L_{ic} + L_{bc}$\n",
        "| Inverse | $(\\boldsymbol{\\mathrm{x}},t)$ | $\\theta,\\lambda$ | $L = L_{pde} + L_{data}$\n",
        "| Ill-posed | $(\\boldsymbol{\\mathrm{x}},t)$ | $\\theta$ | $L = L_{pde} + L_{data}$\n",
        "| Generalization | $(\\boldsymbol{\\mathrm{x}},t,\\lambda)$ | $\\theta$ | $L = L_{pde} + L_{ic} + L_{bc}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "258da9c9",
      "metadata": {
        "id": "258da9c9"
      },
      "source": [
        "## Forward problem\n",
        "In forward problems, the initial and/or boundary conditions (IC and/or BCs) of the PDEs are well-defined. The measurements of the solutions (supervised data) are optional. We aim to infer the solution in the entire domain using PINNs.\n",
        "\n",
        "To solve a PDE problem, there exists two equivalent formulations: the strong form and the weak form.\n",
        "* The strong form consists of the differential equations along with IC and BCs. It imposes continuity and differentiability requirements on the potential solutions to the equation.\n",
        "* The weak form is an alternative representation of the differential equations, which relaxes the continuity and differentiability requirements. It reduces the order of the derivatives and forces the solution to satisfied an integral functions.\n",
        "\n",
        "### Example: Viscous Burgers equation <a name=\"1.1-bullet\"></a>\n",
        "We consider the following viscous Burgers equation:\n",
        "\\begin{align}\n",
        "  &u_t + uu_x -\\nu u_{xx} = 0 \\quad \\text{for } x\\in [0,2], t\\in[0, 1] \\\\\n",
        "  &u(x,0) = -\\sin(\\pi x) \\quad \\text{for } x\\in [0,2]\\\\\n",
        "  &u(0,t) = u(2,t) = 0 \\quad \\text{for } t\\in [0,1]\n",
        "\\end{align}\n",
        "where $\\nu$ is the PDE parameter. In this section, we take $\\nu=0.025$.\n",
        "\n",
        "We denote $\\hat{u}=\\Phi(\\boldsymbol{\\mathrm{x}}, t, \\boldsymbol{\\theta})$ the prediction of PINNs for the solution. The loss function, which includes the loss the IC/BC and the PDE residuals, now becomes:\n",
        "\\begin{align}\n",
        "  L &= L_{pde} + L_{ic} + L_{bc}\\\\\n",
        "    &= \\dfrac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}|\\hat{u}^i_{t} + \\hat{u}^i\\hat{u}^i_{x}-\\nu \\hat{u}_{xx}|^2 + \\dfrac{w_{ic}}{N_{ic}}\\sum_{i=1}^{N_{ic}}|\\hat{u}(x^i,0) + \\sin(\\pi x^i)|^2 + \\dfrac{w_{bc}}{N_{bc}}\\sum_{i=1}^{N_{bc}}(|\\hat{u}(-1,t^i)|^2+|\\hat{u}(1,t^i)|^2)\n",
        "\\end{align}\n",
        "Here we will take $w_{ic}=w_{bc}=1$.\n",
        "\n",
        "We take the following steps to train PINNs:\n",
        "* Define the domain\n",
        "* Define the initial/boundary conditions (IC/BC) and the training points\n",
        "* Define PDE residuals and loss term for the PDE\n",
        "* Define PINNs architecture\n",
        "* Train PINNs\n",
        "\n",
        "#### Define the domain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9850ce4b",
      "metadata": {
        "id": "9850ce4b"
      },
      "source": [
        "## 2. Setting Up the Environment\n",
        "\n",
        "First, let's import the required libraries and set up our computational environment. We'll use PyTorch for the neural network implementation, NumPy for numerical operations, and Matplotlib for visualization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "C_DHItUCvDs0"
      },
      "id": "C_DHItUCvDs0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f9ea68",
      "metadata": {
        "id": "77f9ea68"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Parameters for both viscous and inviscid cases\n",
        "x_min, x_max = 0, 2     # Spatial domain\n",
        "t_min, t_max = 0, 0.48  # Time domain\n",
        "viscosity = 0.01/np.pi  # Viscosity coefficient (will be 0 for inviscid case)\n",
        "\n",
        "# Create directory for saving plots\n",
        "import os\n",
        "os.makedirs('./plots', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de7e8ba1",
      "metadata": {
        "id": "de7e8ba1"
      },
      "source": [
        "## 3. Neural Network Architecture\n",
        "\n",
        "We'll use a fully connected neural network with multiple hidden layers. This architecture will be used for both the viscous and inviscid cases. The network takes (x, t) as input and predicts u(x, t) as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439e647a",
      "metadata": {
        "id": "439e647a"
      },
      "outputs": [],
      "source": [
        "class BurgersPINN(nn.Module):\n",
        "    def __init__(self, layers=[2, 32, 128, 16, 128, 32, 1]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Network layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(layers)-1):\n",
        "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
        "\n",
        "        # Initialize weights with Xavier\n",
        "        for layer in self.layers:\n",
        "            nn.init.xavier_normal_(layer.weight)\n",
        "            nn.init.zeros_(layer.bias)\n",
        "\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "        for i in range(len(self.layers)-1):\n",
        "            x = self.activation(self.layers[i](x))\n",
        "        x = self.layers[-1](x)\n",
        "        return x\n",
        "\n",
        "# Test the network\n",
        "network = BurgersPINN()\n",
        "print(f\"Network architecture:\\n{network}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a2bbb1",
      "metadata": {
        "id": "88a2bbb1"
      },
      "source": [
        "## 4. Viscous Burgers Equation Implementation (Reference)\n",
        "\n",
        "Here's the complete implementation of the PINN solver for the viscous Burgers equation. Study this implementation carefully as you'll need to modify it for the inviscid case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a27fab74",
      "metadata": {
        "id": "a27fab74"
      },
      "outputs": [],
      "source": [
        "# Load reference data from FD solution\n",
        "fd_data = np.load('burgers_fd_data_viscous.npz')\n",
        "x_ref = fd_data['x']\n",
        "t_ref = fd_data['t']\n",
        "u_ref = fd_data['u']\n",
        "\n",
        "def predict_solution(solver, x, t):\n",
        "    \"\"\"Generate predictions for given space-time points.\"\"\"\n",
        "    if not torch.is_tensor(x):\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "    if not torch.is_tensor(t):\n",
        "        t = torch.tensor(t, dtype=torch.float32)\n",
        "\n",
        "    x = x.reshape(-1, 1).to(solver.device)\n",
        "    t = t.reshape(-1, 1).to(solver.device)\n",
        "\n",
        "    X, T = torch.meshgrid(x.squeeze(), t.squeeze(), indexing='ij')\n",
        "    X_pred = torch.stack([X.flatten(), T.flatten()], dim=1).to(solver.device)\n",
        "\n",
        "    solver.network.eval()\n",
        "    with torch.no_grad():\n",
        "        u_pred = solver.network(X_pred)\n",
        "\n",
        "    return u_pred.reshape(x.shape[0], t.shape[0]).cpu().numpy()\n",
        "\n",
        "def plot_comparison(x, t, u_pinn, x_ref, t_ref, u_ref, case=\"viscous\"):\n",
        "    \"\"\"Plot PINN solution against reference data with error analysis.\"\"\"\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    # PINN solution\n",
        "    X, T = np.meshgrid(t, x)\n",
        "    c1 = ax1.contourf(T, X, u_pinn, levels=50, cmap='rainbow')\n",
        "    plt.colorbar(c1, ax=ax1)\n",
        "    ax1.set_xlabel('t')\n",
        "    ax1.set_ylabel('x')\n",
        "    ax1.set_title('PINN Solution')\n",
        "\n",
        "    # Reference solution\n",
        "    X_ref, T_ref = np.meshgrid(t_ref, x_ref)\n",
        "    c2 = ax2.contourf(T_ref, X_ref, u_ref, levels=50, cmap='rainbow')\n",
        "    plt.colorbar(c2, ax=ax2)\n",
        "    ax2.set_xlabel('t')\n",
        "    ax2.set_ylabel('x')\n",
        "    ax2.set_title('Reference Solution (FD)')\n",
        "\n",
        "    # Error plot\n",
        "    # Interpolate reference solution to match PINN grid points\n",
        "    from scipy.interpolate import griddata\n",
        "    points = np.column_stack((T_ref.flatten(), X_ref.flatten()))\n",
        "    u_ref_interp = griddata(points, u_ref.flatten(), (T, X), method='cubic')\n",
        "\n",
        "    error = np.abs(u_pinn - u_ref_interp)\n",
        "    c3 = ax3.contourf(T, X, error, levels=50, cmap='hot')\n",
        "    plt.colorbar(c3, ax=ax3)\n",
        "    ax3.set_xlabel('t')\n",
        "    ax3.set_ylabel('x')\n",
        "    ax3.set_title(f'Absolute Error (Max: {np.nanmax(error):.3e})')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'./plots/comparison_with_reference_{case}.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot time slices\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    times = [0.0, 0.25, 0.48]\n",
        "    time_indices = [np.abs(t - time).argmin() for time in times]\n",
        "    time_indices_ref = [np.abs(t_ref - time).argmin() for time in times]\n",
        "\n",
        "    for i, (time, idx, idx_ref) in enumerate(zip(times, time_indices, time_indices_ref)):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        plt.plot(x, u_pinn[:, idx], 'r-', label='PINN')\n",
        "        plt.plot(x_ref, u_ref[:, idx_ref], 'b--', label='Reference')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('u')\n",
        "        plt.title(f't = {time:.2f}')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'./plots/comparison_slices_with_reference_{case}.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e57e68f7",
      "metadata": {
        "id": "e57e68f7"
      },
      "source": [
        "### Define PINN for the Viscous problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462d7409",
      "metadata": {
        "id": "462d7409"
      },
      "outputs": [],
      "source": [
        "class ViscousBurgersSolver:\n",
        "    def __init__(self, network, device='cpu'):\n",
        "        self.network = network.to(device)\n",
        "        self.device = device\n",
        "\n",
        "        # Generate training points\n",
        "        self.generate_training_data()\n",
        "\n",
        "        # Move data to device\n",
        "        self.X_ic = self.X_ic.to(device)\n",
        "        self.u_ic = self.u_ic.to(device)\n",
        "        self.X_bc_left = self.X_bc_left.to(device)\n",
        "        self.X_bc_right = self.X_bc_right.to(device)\n",
        "        self.X_colloc = self.X_colloc.to(device)\n",
        "\n",
        "    def generate_training_data(self):\n",
        "        # Initial condition points: u(x,0) = sin(πx)\n",
        "        x_ic = torch.linspace(x_min, x_max, 100).reshape(-1, 1)\n",
        "        t_ic = torch.zeros_like(x_ic)\n",
        "        self.X_ic = torch.cat((x_ic, t_ic), dim=1)\n",
        "        self.u_ic = torch.sin(np.pi * x_ic)\n",
        "\n",
        "        # Boundary condition points\n",
        "        t_bc = torch.linspace(t_min, t_max, 100).reshape(-1, 1)\n",
        "        x_bc_left = torch.zeros_like(t_bc)\n",
        "        x_bc_right = torch.ones_like(t_bc) * x_max\n",
        "        self.X_bc_left = torch.cat((x_bc_left, t_bc), dim=1)\n",
        "        self.X_bc_right = torch.cat((x_bc_right, t_bc), dim=1)\n",
        "\n",
        "        # Collocation points for PDE\n",
        "        n_colloc = 10000\n",
        "        x_colloc = torch.rand(n_colloc, 1) * (x_max - x_min) + x_min\n",
        "        t_colloc = torch.rand(n_colloc, 1) * (t_max - t_min) + t_min\n",
        "        self.X_colloc = torch.cat((x_colloc, t_colloc), dim=1)\n",
        "\n",
        "    def compute_pde_residual(self, x_colloc):\n",
        "        x_colloc.requires_grad = True\n",
        "        u = self.network(x_colloc)\n",
        "\n",
        "        # Calculate derivatives\n",
        "        u_grad = torch.autograd.grad(u.sum(), x_colloc, create_graph=True)[0]\n",
        "        u_t = u_grad[:, 1:2]  # ∂u/∂t\n",
        "        u_x = u_grad[:, 0:1]  # ∂u/∂x\n",
        "\n",
        "        # Second derivative ∂²u/∂x²\n",
        "        u_xx = torch.autograd.grad(u_x.sum(), x_colloc, create_graph=True)[0][:, 0:1]\n",
        "\n",
        "        # PDE residual: ∂u/∂t + u*∂u/∂x - ν*∂²u/∂x²\n",
        "        residual = u_t + u * u_x - viscosity * u_xx\n",
        "\n",
        "        return residual\n",
        "\n",
        "    def loss_ic(self, x, u):\n",
        "        u_pred = self.network(x)\n",
        "        return torch.mean((u_pred - u) ** 2)\n",
        "\n",
        "    def loss_bc(self, x):\n",
        "        u_pred = self.network(x)\n",
        "        return torch.mean(u_pred ** 2)\n",
        "\n",
        "    def loss_pde(self, x):\n",
        "        residual = self.compute_pde_residual(x)\n",
        "        return torch.mean(residual ** 2)\n",
        "\n",
        "    def train(self, epochs=10000, learning_rate=0.001):\n",
        "        optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
        "        scheduler = ExponentialLR(optimizer, gamma=0.9999)\n",
        "\n",
        "        history = {'total_loss': [], 'ic_loss': [], 'bc_loss': [], 'pde_loss': []}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute losses\n",
        "            ic_loss = self.loss_ic(self.X_ic, self.u_ic)\n",
        "            bc_loss = self.loss_bc(self.X_bc_left) + self.loss_bc(self.X_bc_right)\n",
        "            pde_loss = self.loss_pde(self.X_colloc)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = ic_loss + bc_loss + pde_loss\n",
        "\n",
        "            # Backpropagation\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Store losses\n",
        "            if epoch % 1000 == 0:\n",
        "                history['total_loss'].append(total_loss.item())\n",
        "                history['ic_loss'].append(ic_loss.item())\n",
        "                history['bc_loss'].append(bc_loss.item())\n",
        "                history['pde_loss'].append(pde_loss.item())\n",
        "                print(f\"Epoch {epoch}: Loss = {total_loss.item():.6f}\")\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdadeee3",
      "metadata": {
        "id": "fdadeee3"
      },
      "source": [
        "### Train PINN for Viscous problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00263631",
      "metadata": {
        "id": "00263631"
      },
      "outputs": [],
      "source": [
        "# Train and plot the Viscous Burgers Equation solution\n",
        "x = np.linspace(x_min, x_max, 100)\n",
        "t = np.linspace(t_min, t_max, 100)\n",
        "\n",
        "# Train the model\n",
        "network_viscous = BurgersPINN().to(device)\n",
        "solver_viscous = ViscousBurgersSolver(network_viscous, device)\n",
        "history_viscous = solver_viscous.train()\n",
        "\n",
        "# Get PINN solution\n",
        "u_pinn = predict_solution(solver_viscous, x, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b4615cf",
      "metadata": {
        "id": "4b4615cf"
      },
      "source": [
        "### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e5bcea",
      "metadata": {
        "id": "51e5bcea"
      },
      "outputs": [],
      "source": [
        "# Compare with reference solution\n",
        "plot_comparison(x, t, u_pinn, x_ref, t_ref, u_ref)\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.semilogy(range(0, len(history_viscous['total_loss'])*100, 100), history_viscous['total_loss'], label='Total Loss')\n",
        "plt.semilogy(range(0, len(history_viscous['ic_loss'])*100, 100), history_viscous['ic_loss'], label='IC Loss')\n",
        "plt.semilogy(range(0, len(history_viscous['bc_loss'])*100, 100), history_viscous['bc_loss'], label='BC Loss')\n",
        "plt.semilogy(range(0, len(history_viscous['pde_loss'])*100, 100), history_viscous['pde_loss'], label='PDE Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training History')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('./plots/training_history.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd7eda76",
      "metadata": {
        "id": "dd7eda76"
      },
      "source": [
        "### Forcing hardly the IC/BC to increase the performance\n",
        "We recall the initial/boundary conditions (IC/BC) of this problem:\n",
        "\\begin{align}\n",
        "  &u(x,0) = \\sin(\\pi x) \\quad \\text{for } x\\in [0,2]\\\\\n",
        "  &u(0,t) = u(2,t) = 0 \\quad \\text{for } t\\in [0,1]\n",
        "\\end{align}\n",
        "\n",
        "We can force the IC/BC to be automatically satisfied. Instead of imposing the prediction $\\hat{\\boldsymbol{u}} = \\Phi(\\boldsymbol{\\mathrm{x}}, t, \\boldsymbol{\\theta})$, we can use the following representation:\n",
        "$$\\hat{\\boldsymbol{u}} = t(x-0)(x-2) \\Phi(\\boldsymbol{\\mathrm{x}}, t, \\boldsymbol{\\theta}) + \\sin(\\pi x)$$\n",
        "We can do this by creating a new class that defines our problem, `ViscousBurgersSolverBC`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4418b010",
      "metadata": {
        "id": "4418b010"
      },
      "outputs": [],
      "source": [
        "class ViscousBurgersSolverBC:\n",
        "    def __init__(self, network, device='cpu'):\n",
        "        self.network = network.to(device)\n",
        "        self.device = device\n",
        "\n",
        "        # Generate training points\n",
        "        self.generate_training_data()\n",
        "\n",
        "        # Move data to device\n",
        "        self.X_ic = self.X_ic.to(device)\n",
        "        self.u_ic = self.u_ic.to(device)\n",
        "        self.X_bc_left = self.X_bc_left.to(device)\n",
        "        self.X_bc_right = self.X_bc_right.to(device)\n",
        "        self.X_colloc = self.X_colloc.to(device)\n",
        "\n",
        "    def generate_training_data(self):\n",
        "        # Initial condition points: u(x,0) = -sin(πx)\n",
        "        x_ic = torch.linspace(x_min, x_max, 100).reshape(-1, 1)\n",
        "        t_ic = torch.zeros_like(x_ic)\n",
        "        self.X_ic = torch.cat((x_ic, t_ic), dim=1)\n",
        "        self.u_ic = torch.sin(np.pi * x_ic)\n",
        "\n",
        "        # Boundary condition points\n",
        "        t_bc = torch.linspace(t_min, t_max, 100).reshape(-1, 1)\n",
        "        x_bc_left = torch.zeros_like(t_bc)\n",
        "        x_bc_right = torch.ones_like(t_bc) * x_max\n",
        "        self.X_bc_left = torch.cat((x_bc_left, t_bc), dim=1)\n",
        "        self.X_bc_right = torch.cat((x_bc_right, t_bc), dim=1)\n",
        "\n",
        "        # Collocation points for PDE\n",
        "        n_colloc = 10000\n",
        "        x_colloc = torch.rand(n_colloc, 1) * (x_max - x_min) + x_min\n",
        "        t_colloc = torch.rand(n_colloc, 1) * (t_max - t_min) + t_min\n",
        "        self.X_colloc = torch.cat((x_colloc, t_colloc), dim=1)\n",
        "\n",
        "    def compute_pde_residual(self, x_colloc):\n",
        "        x_colloc.requires_grad = True\n",
        "        # u should be the result of  t * Network + sin(pi*x)\n",
        "        # This is to ensure the network predicts the solution at t=0 and x=0 and x=2 correctly\n",
        "        # get the vector of t_colloc from x_colloc\n",
        "        tc_colloc = x_colloc[:, 1:2]  # Extract time component\n",
        "        xc_colloc = x_colloc[:, 0:1]  # Extract space component\n",
        "\n",
        "        u = tc_colloc * (xc_colloc - x_max) * (xc_colloc - x_min) * self.network(x_colloc) + torch.sin(np.pi * xc_colloc)\n",
        "\n",
        "        # Calculate derivatives\n",
        "        u_grad = torch.autograd.grad(u.sum(), x_colloc, create_graph=True)[0]\n",
        "        u_t = u_grad[:, 1:2]  # ∂u/∂t\n",
        "        u_x = u_grad[:, 0:1]  # ∂u/∂x\n",
        "\n",
        "        # Second derivative ∂²u/∂x²\n",
        "        u_xx = torch.autograd.grad(u_x.sum(), x_colloc, create_graph=True)[0][:, 0:1]\n",
        "\n",
        "        # PDE residual: ∂u/∂t + u*∂u/∂x - ν*∂²u/∂x²\n",
        "        residual = u_t + u * u_x - viscosity * u_xx\n",
        "\n",
        "        return residual\n",
        "\n",
        "    def loss_pde(self, x):\n",
        "        residual = self.compute_pde_residual(x)\n",
        "        return torch.mean(residual ** 2)\n",
        "\n",
        "    def train(self, epochs=10000, learning_rate=0.001):\n",
        "        optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
        "        scheduler = ExponentialLR(optimizer, gamma=0.9999)\n",
        "\n",
        "        history = {'total_loss': [], 'ic_loss': [], 'bc_loss': [], 'pde_loss': []}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute losses\n",
        "            pde_loss = self.loss_pde(self.X_colloc)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = pde_loss\n",
        "\n",
        "            # Backpropagation\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Store losses\n",
        "            if epoch % 1000 == 0:\n",
        "                history['total_loss'].append(total_loss.item())\n",
        "                history['pde_loss'].append(pde_loss.item())\n",
        "                print(f\"Epoch {epoch}: Loss = {total_loss.item():.6f}\")\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fd5b2e",
      "metadata": {
        "id": "59fd5b2e"
      },
      "source": [
        "### Train PINN for Viscous problem\n",
        "\n",
        "Note that we need a new `predict_solution_bc` function for the boundary condition case, this function will handle the specific boundary conditions for the viscous Burgers equation.\n",
        "\n",
        "Because our network predicts directly not the full solution but the residuals we need to adjust the prediction to account for the boundary conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aaca5af",
      "metadata": {
        "id": "3aaca5af"
      },
      "outputs": [],
      "source": [
        "def predict_solution_bc(solver, x, t):\n",
        "    \"\"\"Generate predictions for given space-time points.\"\"\"\n",
        "    if not torch.is_tensor(x):\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "    if not torch.is_tensor(t):\n",
        "        t = torch.tensor(t, dtype=torch.float32)\n",
        "\n",
        "    x = x.reshape(-1, 1).to(solver.device)\n",
        "    t = t.reshape(-1, 1).to(solver.device)\n",
        "\n",
        "    X, T = torch.meshgrid(x.squeeze(), t.squeeze(), indexing='ij')\n",
        "    X_pred = torch.stack([X.flatten(), T.flatten()], dim=1).to(solver.device)\n",
        "\n",
        "    solver.network.eval()\n",
        "    with torch.no_grad():\n",
        "        tc_colloc = X_pred[:, 1:2]  # Extract time component\n",
        "        xc_colloc = X_pred[:, 0:1]  # Extract space component\n",
        "\n",
        "        u_pred = tc_colloc * (xc_colloc - x_max) * (xc_colloc - x_min) * solver.network(X_pred) + torch.sin(np.pi * xc_colloc)\n",
        "    return u_pred.reshape(x.shape[0], t.shape[0]).cpu().numpy()\n",
        "\n",
        "\n",
        "# Train and plot the Viscous Burgers Equation solution\n",
        "x = np.linspace(x_min, x_max, 100)\n",
        "t = np.linspace(t_min, t_max, 100)\n",
        "\n",
        "# Train the model\n",
        "network_viscous = BurgersPINN().to(device)\n",
        "solver_viscous = ViscousBurgersSolverBC(network_viscous, device)\n",
        "history_viscous = solver_viscous.train()\n",
        "\n",
        "# Get PINN solution\n",
        "u_pinn = predict_solution_bc(solver_viscous, x, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dcf326f",
      "metadata": {
        "id": "6dcf326f"
      },
      "outputs": [],
      "source": [
        "# Compare with reference solution\n",
        "plot_comparison(x, t, u_pinn, x_ref, t_ref, u_ref)\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.semilogy(range(0, len(history_viscous['total_loss'])*100, 100), history_viscous['total_loss'], label='Total Loss')\n",
        "plt.semilogy(range(0, len(history_viscous['pde_loss'])*100, 100), history_viscous['pde_loss'], label='PDE Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training History')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('./plots/training_history.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd58cb4",
      "metadata": {
        "id": "6bd58cb4"
      },
      "source": [
        "## 5. <font color='red'>Exercise</font>: Implement Inviscid Burgers Equation\n",
        "\n",
        "Now it's your turn! Implement the PINN solver for the inviscid Burgers equation. The main differences from the viscous case are:\n",
        "\n",
        "1. No viscosity term in the PDE (ν = 0)\n",
        "2. Different handling of shock formation\n",
        "3. Possible need for more collocation points to handle the discontinuity at the shock\n",
        "4. Possible need to handle the boundary conditions exactly\n",
        "\n",
        "Here's a template to get you started. Complete the `compute_pde_residual` method and any other modifications you think are necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d126e1c",
      "metadata": {
        "id": "8d126e1c"
      },
      "outputs": [],
      "source": [
        "class InviscidBurgersSolver:\n",
        "    def __init__(self, network, device='cpu'):\n",
        "        self.network = network.to(device)\n",
        "        self.device = device\n",
        "        self.generate_training_data()\n",
        "\n",
        "        # Move data to device\n",
        "        self.X_ic = self.X_ic.to(device)\n",
        "        self.u_ic = self.u_ic.to(device)\n",
        "        self.X_bc_left = self.X_bc_left.to(device)\n",
        "        self.X_bc_right = self.X_bc_right.to(device)\n",
        "        self.X_colloc = self.X_colloc.to(device)\n",
        "\n",
        "    def generate_training_data(self):\n",
        "        # TODO: Modify if needed for inviscid case\n",
        "        # Initial condition points\n",
        "        x_ic = torch.linspace(x_min, x_max, 100).reshape(-1, 1)\n",
        "        t_ic = torch.zeros_like(x_ic)\n",
        "        self.X_ic = torch.cat((x_ic, t_ic), dim=1)\n",
        "        self.u_ic = torch.sin(np.pi * x_ic)\n",
        "\n",
        "        # Boundary conditions\n",
        "        t_bc = torch.linspace(t_min, t_max, 100).reshape(-1, 1)\n",
        "        x_bc_left = torch.zeros_like(t_bc)\n",
        "        x_bc_right = torch.ones_like(t_bc) * x_max\n",
        "        self.X_bc_left = torch.cat((x_bc_left, t_bc), dim=1)\n",
        "        self.X_bc_right = torch.cat((x_bc_right, t_bc), dim=1)\n",
        "\n",
        "        # Collocation points\n",
        "        n_colloc = 10000\n",
        "        x_colloc = torch.rand(n_colloc, 1) * (x_max - x_min) + x_min\n",
        "        t_colloc = torch.rand(n_colloc, 1) * (t_max - t_min) + t_min\n",
        "        self.X_colloc = torch.cat((x_colloc, t_colloc), dim=1)\n",
        "\n",
        "    def compute_pde_residual(self, x_colloc):\n",
        "        # TODO: Implement the PDE residual for inviscid Burgers equation\n",
        "        # Hint: You need to compute ∂u/∂t + u*∂u/∂x = 0\n",
        "\n",
        "        # TODO: Implement the residual\n",
        "        residual = None  # Replace this line\n",
        "\n",
        "        return residual\n",
        "\n",
        "    # TODO: Implement any additional loss terms needed\n",
        "\n",
        "    def train(self, epochs=10000, learning_rate=0.001):\n",
        "        # TODO: Modify the training loop if needed\n",
        "        pass  # Replace this with your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38582d54",
      "metadata": {
        "id": "38582d54"
      },
      "source": [
        "## 5. Training and Comparison Functions\n",
        "\n",
        "Here are helper functions to train both models and compare their results. After implementing the inviscid solver, use these functions to analyze the differences between the two solutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614d5828",
      "metadata": {
        "id": "614d5828"
      },
      "outputs": [],
      "source": [
        "def predict_solution(solver, x, t):\n",
        "    \"\"\"Generate predictions for given space-time points.\"\"\"\n",
        "    if not torch.is_tensor(x):\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "    if not torch.is_tensor(t):\n",
        "        t = torch.tensor(t, dtype=torch.float32)\n",
        "\n",
        "    x = x.reshape(-1, 1).to(solver.device)\n",
        "    t = t.reshape(-1, 1).to(solver.device)\n",
        "\n",
        "    X, T = torch.meshgrid(x.squeeze(), t.squeeze(), indexing='ij')\n",
        "    X_pred = torch.stack([X.flatten(), T.flatten()], dim=1).to(solver.device)\n",
        "\n",
        "    solver.network.eval()\n",
        "    with torch.no_grad():\n",
        "        u_pred = solver.network(X_pred)\n",
        "\n",
        "    return u_pred.reshape(x.shape[0], t.shape[0]).cpu().numpy()\n",
        "\n",
        "def plot_solutions(x, t, u_viscous, u_inviscid):\n",
        "    \"\"\"Plot and compare viscous and inviscid solutions.\"\"\"\n",
        "    # Create time slices plot\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    times = [0.0, 0.25, 0.48]\n",
        "    time_indices = [np.abs(t - time).argmin() for time in times]\n",
        "\n",
        "    for i, (time, idx) in enumerate(zip(times, time_indices)):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        plt.plot(x, u_viscous[:, idx], 'r-', label='Viscous')\n",
        "        plt.plot(x, u_inviscid[:, idx], 'b--', label='Inviscid')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('u')\n",
        "        plt.title(f't = {time:.2f}')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('./plots-tutorial/comparison_slices.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Create contour plots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    X, T = np.meshgrid(t, x)\n",
        "\n",
        "    c1 = ax1.contourf(T, X, u_viscous, levels=50, cmap='rainbow')\n",
        "    plt.colorbar(c1, ax=ax1)\n",
        "    ax1.set_xlabel('t')\n",
        "    ax1.set_ylabel('x')\n",
        "    ax1.set_title('Viscous Solution')\n",
        "\n",
        "    c2 = ax2.contourf(T, X, u_inviscid, levels=50, cmap='rainbow')\n",
        "    plt.colorbar(c2, ax=ax2)\n",
        "    ax2.set_xlabel('t')\n",
        "    ax2.set_ylabel('x')\n",
        "    ax2.set_title('Inviscid Solution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('./plots-tutorial/comparison_contours.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dba09f1",
      "metadata": {
        "id": "4dba09f1"
      },
      "source": [
        "## 6. Example Usage and Analysis\n",
        "\n",
        "Here's an example of how to use the completed implementation. After implementing the inviscid solver, run this code to compare the solutions.\n",
        "\n",
        "### Expected Observations:\n",
        "1. The viscous solution will be smooth, while the inviscid solution may develop shocks\n",
        "2. The inviscid case will show steeper gradients and more abrupt changes\n",
        "3. The viscous term acts to smooth out discontinuities\n",
        "\n",
        "Try different initial conditions and parameter values to explore the behavior of both equations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3edb3d95",
      "metadata": {
        "id": "3edb3d95"
      },
      "outputs": [],
      "source": [
        "# Generate space-time grid for predictions\n",
        "x = np.linspace(x_min, x_max, 100)\n",
        "t = np.linspace(t_min, t_max, 100)\n",
        "\n",
        "# Train viscous solver\n",
        "network_viscous = BurgersPINN().to(device)\n",
        "solver_viscous = ViscousBurgersSolver(network_viscous, device)\n",
        "history_viscous = solver_viscous.train()\n",
        "\n",
        "# TODO: Uncomment after implementing InviscidBurgersSolver\n",
        "\"\"\"\n",
        "# Train inviscid solver\n",
        "network_inviscid = BurgersPINN().to(device)\n",
        "solver_inviscid = InviscidBurgersSolver(network_inviscid, device)\n",
        "history_inviscid = solver_inviscid.train()\n",
        "\n",
        "# Generate predictions\n",
        "u_viscous = predict_solution(solver_viscous, x, t)\n",
        "u_inviscid = predict_solution(solver_inviscid, x, t)\n",
        "\n",
        "# Plot and compare solutions\n",
        "plot_solutions(x, t, u_viscous, u_inviscid)\n",
        "\"\"\"\n",
        "\n",
        "# For now, just plot viscous solution\n",
        "plt.figure(figsize=(10, 6))\n",
        "u_viscous = predict_solution(solver_viscous, x, t)\n",
        "X, T = np.meshgrid(t, x)\n",
        "plt.contourf(T, X, u_viscous, levels=50, cmap='rainbow')\n",
        "plt.colorbar(label='u')\n",
        "plt.xlabel('t')\n",
        "plt.ylabel('x')\n",
        "plt.title('Viscous Burgers Equation Solution')\n",
        "plt.savefig('./plots-tutorial/viscous_solution.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's compare our predicted solution for the inviscid case against finite-difference data."
      ],
      "metadata": {
        "id": "rilaFn6C5sDy"
      },
      "id": "rilaFn6C5sDy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load inviscid reference data from FD solution\n",
        "# This is a much larger file, so it will take longer to upload!\n",
        "# We also need to take a sample of the data since it's unlikely to fit in memory.\n",
        "rng = np.random.default_rng()\n",
        "inviscid_fd_data = np.load('burgers_fd_data_inviscid.npz')\n",
        "\n",
        "# Assume u shape is (len(x), len(t))\n",
        "inviscid_x_ref = inviscid_fd_data['x']\n",
        "inviscid_t_ref = inviscid_fd_data['t']\n",
        "inviscid_u_ref = inviscid_fd_data['u']\n",
        "\n",
        "# Define subsampling step\n",
        "subsample_step_x = 4  # e.g., every 4th x-point\n",
        "subsample_step_t = 5  # e.g., every 5th t-point\n",
        "\n",
        "# Subsample the data\n",
        "subsampled_x = inviscid_x_ref[::subsample_step_x]\n",
        "subsampled_t = inviscid_t_ref[::subsample_step_t]\n",
        "subsampled_u = inviscid_u_ref[::subsample_step_x, ::subsample_step_t]"
      ],
      "metadata": {
        "id": "yGIMZzjJ5jqc"
      },
      "id": "yGIMZzjJ5jqc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_comparison(x, t, u_pinn, subsampled_x, subsampled_t, subsampled_u, case=\"inviscid\")"
      ],
      "metadata": {
        "id": "h9Ny0Hxw5sli"
      },
      "id": "h9Ny0Hxw5sli",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6hCU8ZNM6cay"
      },
      "id": "6hCU8ZNM6cay",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}